{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from smart_open import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracttext(text):\n",
    "    \n",
    "    sentenses = re.split(r\"[\\n|\\r]\", text)\n",
    "    # Remove leading and trailing spaces from each sentence\n",
    "    results = []\n",
    "    for sen in sentenses:\n",
    "        s = sen.strip()\n",
    "        if len(s):\n",
    "            results.append(s)\n",
    "            num_libros=len(results)\n",
    "    #print(\"\\n\")\n",
    "    return results[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querysearch():\n",
    "    \n",
    "    queryid = []\n",
    "    querytext = []\n",
    "    a = 1\n",
    "    queryall = [queryid ,querytext]\n",
    "    while a!=47:\n",
    "        if a < 10:\n",
    "            b = \"0\"+str(a)\n",
    "            if a == 4:\n",
    "                a+=1\n",
    "        elif a < 50:\n",
    "                b = str(a)\n",
    "                if a == 10 or a == 14 or a == 32 or a == 34 or a ==38 or a == 42:\n",
    "                    a+=1\n",
    "                elif a == 19 or a == 29:\n",
    "                    a+=2\n",
    "\n",
    "        docquer = etree.parse(\"./queries-raw-texts/wes2015.q\"+b+\".naf\")\n",
    "        raizquer=docquer.getroot()\n",
    "        idquer = raizquer[0][1].get('publicId')\n",
    "        textquery = raizquer[1].text\n",
    "        queryid.append(idquer)\n",
    "        querytext.append(textquery)\n",
    "        a+=1\n",
    "    #print(queryid)\n",
    "    #print(querytext)    \n",
    "    return queryall\n",
    "    #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim trae sus propias funciones para el procesamiento de texto\n",
    "p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Corpus Streaming</h2>\n",
    "Normalmente los corpus reciden completamente en la memoria. Supongamos que hay millones de documentos en el corpus. Almacenarlos todos en RAM no será suficiente. En su lugar, supongamos que los documentos se almacenan en un archivo en el disco, un documento por línea. Gensim puede procesar un documento a la vez y actualizar dinamicamente e calculo de IDF y del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesamiento de un texto utilizando las funciones de Gensim\n",
    "def process(text):\n",
    "    doc_nor = text.lower()\n",
    "    #doc_reg = re.sub(r'[^\\w|^\\d]', ' ', doc_nor)\n",
    "    #doc_reg = re.sub(r'[^a-zA-Z]', ' ', doc_nor)\n",
    "    doc_sw = remove_stopwords(doc_nor)\n",
    "    doc_stem = PorterStemmer().stem_sentence(doc_sw)\n",
    "    return doc_stem.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creaccion de lista con documentos procesados\n",
    "docDict = []\n",
    "docDict1 = []\n",
    "docDict2 = []\n",
    "docTitle = []\n",
    "docID = []\n",
    "a = 1\n",
    "\n",
    "while a!=332:\n",
    "    if a < 10:\n",
    "        b = \"00\"+str(a)\n",
    "    elif a < 100:\n",
    "            b = \"0\"+str(a)\n",
    "    else:\n",
    "            b = str(a)\n",
    "    doc = etree.parse(\"./docs-raw-texts/wes2015.d\"+b+\".naf\")\n",
    "    raiz=doc.getroot()\n",
    "    titulo = raiz[0][0].get('title')\n",
    "    docTitle.append(titulo)\n",
    "    iddoc = raiz[0][1].get('publicId')\n",
    "    docID.append(iddoc)\n",
    "    #print(iddoc)\n",
    "    #print(titulo)\n",
    "    text = raiz[1].text\n",
    "    a+=1\n",
    "    line = extracttext(text)\n",
    "    #print(\"\\n\")\n",
    "    docDict.append(process(line))\n",
    "\n",
    "#print(docID[2])\n",
    "#print(docTitle[2])\n",
    "#print(docDict[2]) #Documentos ya procesados, de esta forma deben ser enviados al metodo de Gensim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora si llamamos al metodo de Gensim para crear el diccionario a partir de los documentos\n",
    "dictionary = corpora.Dictionary(docDict)\n",
    "dictionary.save('clase3.dict')\n",
    "#print(dictionary)\n",
    "\n",
    "#Notese que Gensim de una vez asigna a cada token un id\n",
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(70, 1), (4236, 1), (7809, 2)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human media interaction media\"\n",
    "new_doc_bow = dictionary.doc2bow(process(new_doc))\n",
    "print(new_doc_bow)  # ¿Si hay cuatro palabras pq solo dos tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construccion del corpus == Cada documento representado con el vocabulario/diccionario definido anteriormente (Bolsa de palabras)\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        a = 1\n",
    "\n",
    "        while a!=332:\n",
    "            if a < 10:\n",
    "                b = \"00\"+str(a)\n",
    "            elif a < 100:\n",
    "                    b = \"0\"+str(a)\n",
    "            else:\n",
    "                b = str(a)\n",
    "            doc = etree.parse(\"./docs-raw-texts/wes2015.d\"+b+\".naf\")\n",
    "            raiz=doc.getroot()\n",
    "            titulo = raiz[0][0].get('title')\n",
    "            iddoc = raiz[0][1].get('publicId')\n",
    "            #print(iddoc)\n",
    "            #print(titulo)\n",
    "            text = raiz[1].text\n",
    "            a+=1\n",
    "            line = extracttext(text)\n",
    "            yield dictionary.doc2bow(process(line))\n",
    "\n",
    "corpus_memory_friendly = MyCorpus()#Todos mis documentos ahora estan representados como una bolsa de palabras\n",
    "#Almaceno mi corpus\n",
    "corpora.MmCorpus.serialize('corpus.mm', corpus_memory_friendly)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargo mi corpus\n",
    "corpus = corpora.MmCorpus('corpus.mm')\n",
    "#print(corpus) #No lo carga en memoria\n",
    "#Para leer la representacion de bolda de palabras resultante de cada documento en el corpus\n",
    "#for doc in corpus:\n",
    "#    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vector Space Model - TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suponga que tenemos el diccionario y corpus en disco\n",
    "#Primero necesitamos cargarlos\n",
    "dictionary = corpora.Dictionary.load('clase3.dict')\n",
    "corpus = corpora.MmCorpus('corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construyendo un modelo VSM con ponderacion por TF-IDF\n",
    "tfidf = models.TfidfModel(corpus) #Inizializacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16449, 1)]\n",
      "[(16449, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "#Supongamos que queremos calcular la similitud de un nuevo documento con los documentos en el corpus\n",
    "#1. Construimos la representación vectorial del query\n",
    "query_doc = \"Amazon vs Microsoft\"\n",
    "query_doc_bow = dictionary.doc2bow(process(query_doc))\n",
    "print(query_doc_bow)\n",
    "print(tfidf[query_doc_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Ahora construimos la matrix t/d con los documentos contra los cuales queremos compara el query\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus]) \n",
    "# Vamos a salvar el inidice resultante para no tener que recalcularlo cada vez que los necesitemos.\n",
    "index.save('clase3tfidf.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una vez guardado lo podemos cargar sin necesidad de recalcularlo\n",
    "index = similarities.MatrixSimilarity.load('clase3tfidf.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar las similitudes entre el documento query y el corpus ahora es muy facil!!!\n",
    "\n",
    "queryreturn = querysearch()\n",
    "queryid = queryreturn[0]\n",
    "querydoc = queryreturn[1]\n",
    "archivo = open ('top-10.txt','w')\n",
    "id=0\n",
    "for query in querydoc:\n",
    "    \n",
    "    query_doc_bow = dictionary.doc2bow(process(query))\n",
    "    sims = index[tfidf[query_doc_bow]]\n",
    "    #print(type(sims))\n",
    "    listofdoctf= list(enumerate(sims))\n",
    "    #print(listofdoctf)\n",
    "    #print(\"\\n\")\n",
    "    sorted_by_second = sorted(listofdoctf,key=itemgetter(1), reverse = True)\n",
    "    #print(sorted_by_second)\n",
    "    varTen = \"10 most similarity documents:\\n\"\n",
    "    #archivo.write(varTen)\n",
    "    archivo.write(queryid[id]+\"\\t\")\n",
    "    for doc, similitudQ in sorted_by_second[:10]:  \n",
    "        archivo.write( docID[doc]+\",\")\n",
    "        #print(\"{}\\t{}\".format(docID[doc], similitudQ))\n",
    "    #print(\"\\n\")    \n",
    "    archivo.write(\"\\n\")\n",
    "    id+=1\n",
    "archivo.close()\n",
    "    \n",
    "\n",
    "#print(list(enumerate(sims)))  print (document_number, document_similarity) 2-tuples#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in smart_open('relevance-judgments.tsv', 'rb'):\n",
    "    docDict2.append(process(line))  \n",
    "#print(docDict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d186:4,d254:5,d016:5'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data22 = docDict2[0][1]\n",
    "data22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d186', 'd254', 'd016']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = re.sub(r':\\d', '', data22)\n",
    "sentence = re.split(\",\",sentence)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d254'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['q01', 'd016,d259,d209,d254,d085,d186,d170,d008,d315,d004,'], ['q02', 'd147,d283,d002,d134,d293,d023,d014,d282,d143,d316,'], ['q03', 'd283,d152,d001,d002,d003,d004,d005,d006,d007,d008,'], ['q04', 'd270,d275,d310,d019,d010,d049,d100,d060,d205,d111,'], ['q06', 'd297,d329,d026,d029,d257,d025,d233,d166,d154,d023,'], ['q07', 'd289,d146,d004,d034,d266,d123,d098,d284,d042,d234,'], ['q08', 'd110,d251,d292,d106,d107,d180,d108,d246,d117,d235,'], ['q09', 'd217,d198,d223,d085,d177,d330,d299,d069,d175,d065,'], ['q10', 'd060,d231,d100,d036,d052,d031,d072,d091,d034,d314,'], ['q12', 'd277,d258,d239,d250,d056,d132,d081,d078,d012,d096,'], ['q13', 'd272,d049,d276,d056,d258,d239,d277,d043,d286,d241,'], ['q14', 'd145,d002,d091,d121,d081,d122,d030,d180,d117,d133,'], ['q16', 'd132,d184,d250,d176,d277,d239,d081,d024,d078,d156,'], ['q17', 'd271,d172,d146,d091,d121,d024,d171,d274,d183,d316,'], ['q18', 'd192,d194,d201,d230,d207,d210,d111,d216,d223,d222,'], ['q19', 'd179,d323,d004,d273,d021,d102,d001,d330,d069,d320,'], ['q22', 'd239,d266,d056,d049,d297,d327,d277,d258,d331,d159,'], ['q23', 'd219,d276,d026,d007,d318,d245,d221,d173,d107,d228,'], ['q24', 'd220,d223,d061,d216,d222,d099,d192,d213,d202,d201,'], ['q25', 'd166,d128,d020,d156,d023,d328,d265,d167,d001,d002,'], ['q26', 'd152,d291,d143,d171,d131,d293,d095,d206,d147,d241,'], ['q27', 'd103,d316,d054,d051,d056,d116,d211,d067,d187,d166,'], ['q28', 'd316,d056,d094,d291,d066,d279,d322,d035,d136,d102,'], ['q29', 'd314,d113,d001,d120,d133,d261,d046,d062,d191,d286,'], ['q32', 'd090,d025,d139,d067,d092,d282,d140,d254,d305,d141,'], ['q34', 'd224,d312,d075,d198,d113,d221,d037,d040,d232,d246,'], ['q36', 'd257,d321,d277,d023,d247,d150,d328,d265,d107,d020,'], ['q37', 'd169,d062,d327,d141,d294,d001,d002,d003,d004,d005,'], ['q38', 'd239,d263,d056,d229,d036,d294,d138,d118,d258,d317,'], ['q40', 'd250,d283,d307,d122,d042,d288,d228,d046,d058,d131,'], ['q41', 'd174,d128,d150,d268,d195,d014,d209,d272,d223,d330,'], ['q42', 'd298,d218,d125,d174,d216,d224,d314,d203,d208,d241,'], ['q44', 'd239,d029,d164,d185,d246,d314,d182,d059,d067,d117,'], ['q45', 'd105,d185,d085,d126,d164,d170,d188,d254,d204,d100,'], ['q46', 'd145,d133,d013,d321,d122,d121,d091,d030,d081,d125,']]\n"
     ]
    }
   ],
   "source": [
    "for line in smart_open('top-10.txt', 'rb'):\n",
    "    docDict1.append(process(line))  \n",
    "print(docDict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d016,d259,d209,d254,d085,d186,d170,d008,d315,d004,'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data12 = docDict1[0][1]\n",
    "data12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d016',\n",
       " 'd259',\n",
       " 'd209',\n",
       " 'd254',\n",
       " 'd085',\n",
       " 'd186',\n",
       " 'd170',\n",
       " 'd008',\n",
       " 'd315',\n",
       " 'd004',\n",
       " '']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = re.split(\",\",data12)\n",
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d259'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
