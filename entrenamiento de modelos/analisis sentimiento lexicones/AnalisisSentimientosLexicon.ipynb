{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Lectura y guardado de los documentos en un data frame de pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# create_new_file: 0 para usar un archivo existente si es posible\n",
    "# o 1 para crear de nuevo\n",
    "# balance_data: 0 para retornar dataframe sin balancear o 1 para retornar\n",
    "# dataframe balanceado.\n",
    "# lematizacion: lematiza los textos usando nltk 0 para no lematizar y 1\n",
    "# para retornar el texto lematizado.\n",
    "# training: une al dataset los datos etiquetados por los modelos, 1 para unir\n",
    "# y 0 para usar solo datos certificados.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_feelings_df(create_new_file, balance_data, lematizacion, training):\n",
    "    from gensim.utils import any2unicode as unicode\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from sklearn.utils import resample\n",
    "    import os, re, sys, pandas, unidecode\n",
    "\n",
    "    feelings_folder = '../datos sentimientos'\n",
    "    feelings_df_path = feelings_folder + '/dataset listo/sentiments_full_df.csv'\n",
    "    feelings_df = pandas.DataFrame(columns=['text', 'lang', 'sentiment'])\n",
    "\n",
    "    if create_new_file == 0 and os.path.exists(feelings_df_path) == True:\n",
    "        print('Cargando datos')\n",
    "        feelings_df = pandas.read_csv(feelings_df_path, encoding='utf8', dtype=str, engine='python')\n",
    "\n",
    "    elif create_new_file == 1 or os.path.exists(feelings_df_path) == False:\n",
    "        path, subfolders, files_list = list(os.walk(feelings_folder))[0]\n",
    "        files_list.sort()\n",
    "        for i in range(len(files_list)):\n",
    "            sys.stdout.write(\n",
    "                \"\\rPreparando dataframe \" + str(round(((i + 1) / (len(files_list))) * 100, 2)) + \"%\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "            file_name, file_ext = files_list[i].split(\".\")\n",
    "\n",
    "            if file_ext == 'csv':\n",
    "                file_path = path + \"/\" + file_name + \".\" + file_ext\n",
    "                df = pandas.read_csv(file_path, encoding='utf8', dtype=str, engine='python')\n",
    "                numero_de_archivo = int(file_name.split(\"_\")[0])\n",
    "\n",
    "                if numero_de_archivo == 1:\n",
    "                    df = df.filter(['airline_sentiment', 'text'])\n",
    "                    df.columns = ['sentiment', 'text']\n",
    "                    df['lang'] = 'en'\n",
    "                    df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                    feelings_df = feelings_df.append(df)\n",
    "\n",
    "                if numero_de_archivo == 2 or numero_de_archivo == 3 or numero_de_archivo == 4:\n",
    "                    df = df.filter(['sentiment', 'text'])\n",
    "                    df['lang'] = 'en'\n",
    "                    df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                    feelings_df = feelings_df.append(df)\n",
    "\n",
    "                if numero_de_archivo == 5 or numero_de_archivo == 6 or numero_de_archivo == 7:\n",
    "                    df = df.filter(['polarity', 'text'])\n",
    "                    df.columns = ['sentiment', 'text']\n",
    "                    if numero_de_archivo in [5, 6]: df['lang'] = 'es'\n",
    "                    if numero_de_archivo == 7: df['lang'] = 'en'\n",
    "                    df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                    feelings_df = feelings_df.append(df)\n",
    "\n",
    "                if numero_de_archivo == 8:\n",
    "                    df = df.filter(['sentiment', 'text'])\n",
    "                    df.columns = ['sentiment', 'text']\n",
    "                    df['lang'] = 'es'\n",
    "                    df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                    feelings_df = feelings_df.append(df)\n",
    "\n",
    "        del df\n",
    "        print(\"\")\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            sys.stdout.write(\n",
    "                \"\\rNormalizacion de datos completada al \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) +\n",
    "                \"%\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "            feelings_df.at[i, 'text'] = (\n",
    "                re.sub(' +', ' ', re.sub(\"http\\S+\", \"\", re.sub('\\s+', ' ', str(feelings_df.at[i, 'text']))))\n",
    "            ).strip()\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    feelings_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    feelings_df = feelings_df.loc[feelings_df['sentiment'].isin(['0', '1', '2'])]\n",
    "    feelings_df = feelings_df.loc[feelings_df['lang'].isin(['es', 'en'])]\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "    feelings_df.to_csv(feelings_df_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    if training == 1 and os.path.exists('./dataset etiquetado modelos/taged_tweets_sample.csv'):\n",
    "        print(\"Incrementando datos con dataset etiquetado por modelos\")\n",
    "        df = pandas.read_csv('./dataset etiquetado modelos/taged_tweets_sample.csv',\n",
    "                             encoding='utf8', dtype=str, engine='python')\n",
    "        df = df.filter(['text', 'lang', 'sentiment'])\n",
    "        feelings_df = feelings_df.append(df)\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "        del df\n",
    "\n",
    "    if lematizacion == 1:\n",
    "        stemmer_en = SnowballStemmer('english')\n",
    "        stemmer_es = SnowballStemmer('spanish')\n",
    "\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            sys.stdout.write(\n",
    "                \"\\rLematizando \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            if feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'es':\n",
    "                feelings_df.at[i, 'text'] = stemmer_es.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "\n",
    "            elif feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'en':\n",
    "                feelings_df.at[i, 'text'] = stemmer_en.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "\n",
    "        print(\"\\nLematizacion finalizada\")\n",
    "\n",
    "    if balance_data == 1:\n",
    "        print(\"Balanceando datos\")\n",
    "        feelings_df[\"Sello\"] = 0\n",
    "\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            sys.stdout.write(\n",
    "                \"\\rCreando sellos de balanceamiento \" +\n",
    "                str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2))\n",
    "                + \"%\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "            feelings_df.at[i, 'sello'] = str(feelings_df.at[i, 'lang']) + '_' + str(feelings_df.at[i, 'sentiment'])\n",
    "\n",
    "        min_len1 = int(min(feelings_df['sello'].value_counts()))\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Balanceando\")\n",
    "\n",
    "        df_0 = resample(feelings_df[feelings_df.sello == 'en_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_1 = resample(feelings_df[feelings_df.sello == 'en_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_2 = resample(feelings_df[feelings_df.sello == 'en_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_3 = resample(feelings_df[feelings_df.sello == 'es_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_4 = resample(feelings_df[feelings_df.sello == 'es_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_5 = resample(feelings_df[feelings_df.sello == 'es_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "\n",
    "        feelings_df = pandas.concat([df_0, df_1, df_2, df_3, df_4, df_5])\n",
    "\n",
    "        print(\"Eliminando sellos de balanceamiento\")\n",
    "\n",
    "        feelings_df = feelings_df.filter(['text', 'lang', 'sentiment'])\n",
    "        print(\"Datos entregados\")\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "        return feelings_df\n",
    "\n",
    "    elif balance_data == 0:\n",
    "        print(\"Datos entregados\")\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "        return feelings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos\n",
      "Lematizando 9.3%%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 25.26%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 41.8%%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 58.42%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 74.32%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 90.33%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando 100.0%\n",
      "Lematizacion finalizada\n",
      "Balanceando datos\n",
      "Creando sellos de balanceamiento 6.31%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando sellos de balanceamiento 22.38%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando sellos de balanceamiento 38.52%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando sellos de balanceamiento 54.07%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando sellos de balanceamiento 72.21%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando sellos de balanceamiento 90.51%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_feelings_df(0,1,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets en el dataframe original: 15342\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTweets en el dataframe original: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JetBlue great flight</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@united Sivi Stewart at Lax was fantastic toni...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@AmericanAir 1138 got us to LGA safely. Thanks...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@united I should be fine. They automatically c...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In view of shortage of Personal Protective Equ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang sentiment\n",
       "0                              @JetBlue great flight   en         0\n",
       "1  @united Sivi Stewart at Lax was fantastic toni...   en         0\n",
       "2  @AmericanAir 1138 got us to LGA safely. Thanks...   en         0\n",
       "3  @united I should be fine. They automatically c...   en         0\n",
       "4  In view of shortage of Personal Protective Equ...   en         0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Lexicones\n",
    "#Lectura de SentiWordNet Obtenido de \n",
    "#https://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html\n",
    "\n",
    "# SentiWordNet[word] = {POS,\tID,\tPosScore,\tNegScore}\n",
    "contador = 0\n",
    "SentiWordNet = dict()\n",
    "for lines in open('Data Lexicon/SentiWordNet_3.0.0.txt'):\n",
    "    if lines.startswith('#'):\n",
    "        continue\n",
    "    line = lines.split('\\t')\n",
    "    palabra = line[4].split('#')[0]\n",
    "    if (palabra in SentiWordNet) or (palabra==''):\n",
    "        continue\n",
    "    else:\n",
    "        SentiWordNet[palabra]={'POS': line[0], 'ID': line[1], 'PosScore': line[2], 'NegScore': line[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFIN[word] = sentiment\n",
    "AFFIN = dict()\n",
    "for lines in open('Data Lexicon/AFFIN-111.txt'):\n",
    "    AFFIN[lines.split('\\t')[0]]=(lines.split('\\t')[1]).split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "def Lexicon(data):\n",
    "    addlex = list()\n",
    "    for frase in data:\n",
    "        splited = tt.tokenize(frase)\n",
    "        sum_swn_neg = 0\n",
    "        sum_swn_pos = 0\n",
    "        affin = 0\n",
    "        stnet = 0\n",
    "        word_stat = 0\n",
    "        for word in splited:\n",
    "            if word in SentiWordNet.keys():\n",
    "                sum_swn_neg += float(SentiWordNet[word]['NegScore'])\n",
    "                sum_swn_pos += float(SentiWordNet[word]['PosScore'])\n",
    "            if word in AFFIN.keys():\n",
    "                affin += float(AFFIN[word])\n",
    "            \n",
    "        addlex.append([sum_swn_neg, sum_swn_pos, affin])\n",
    "    return addlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extension(matriz, data):\n",
    "    extendLex = np.array(Lexicon(data))\n",
    "    return np.append(matriz, extendLex, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=3000, min_df=7, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(df['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizacionLexicon = extension(features, df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in vectorizacionLexicon: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test = train_test_split(vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metricas obtenidas usando AdaBoostClassifier con 6 modelos y una profundidad maxima de 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=200,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=10, random_state=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- implementacion del modelo.\n",
    "predictions0 = modelo.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos para la prueba: 3836\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- resultados del modelo.\n",
    "datos_totales_prueba = len(label_test)\n",
    "print(\"Numero de datos para la prueba: \" + str(datos_totales_prueba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de aciertos totales: 2954\n",
      "Precision total: 0.7700729927007299\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(label_test, predictions0)\n",
    "aciertos = sum([matrix[i][i] for i in range(matrix.shape[0])])\n",
    "precision = aciertos / datos_totales_prueba\n",
    "print(\"Numero de aciertos totales: \" + str(aciertos))\n",
    "print(\"Precision total: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos para la prueba: 3836\n",
      "Precision promedio: 0.769389967958389\n",
      "Recall promedio: 0.7713147258501253\n",
      "\n",
      "Matriz de confusion: \n",
      "+----------+------------+------------+-----------+\n",
      "|          |   positive |   negative |   neutral |\n",
      "+==========+============+============+===========+\n",
      "| positive |        933 |         88 |       229 |\n",
      "+----------+------------+------------+-----------+\n",
      "| negative |         96 |       1068 |       168 |\n",
      "+----------+------------+------------+-----------+\n",
      "| neutral  |        178 |        123 |       953 |\n",
      "+----------+------------+------------+-----------+\n",
      "\n",
      "Metricas de desempe単o: \n",
      "+----------+-------------+----------+\n",
      "|          |   precision |   recall |\n",
      "+==========+=============+==========+\n",
      "| positive |    0.7464   | 0.772991 |\n",
      "+----------+-------------+----------+\n",
      "| negative |    0.801802 | 0.835027 |\n",
      "+----------+-------------+----------+\n",
      "| neutral  |    0.759968 | 0.705926 |\n",
      "+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def imprimir_metricas_matriz_confusion_multiclase(matrix, headers, total_datos):\n",
    "    print(\"Numero de datos para la prueba: \" + str(total_datos))\n",
    "    metrics = np.zeros([len(headers), 2])\n",
    "    for i in range(matrix.shape[0]):\n",
    "        precision = matrix[i][i] / sum(matrix[i])\n",
    "        recoil = matrix[i][i] / sum([matrix[y][i] for y in range(matrix.shape[0])])\n",
    "        metrics[i][0] = precision\n",
    "        metrics[i][-1] = recoil\n",
    "    precision_promedio = sum([metrics[y][0] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    recall_promedio = sum([metrics[y][-1] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    print(\"Precision promedio: \" + str(precision_promedio))\n",
    "    print(\"Recall promedio: \" + str(recall_promedio))\n",
    "    print(\"\\nMatriz de confusion: \\n\" + str(tabulate(\n",
    "        matrix,\n",
    "        headers=headers,\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\\nMetricas de desempe単o: \\n\" + str(tabulate(\n",
    "        metrics,\n",
    "        headers=['precision', 'recall'],\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "imprimir_metricas_matriz_confusion_multiclase(\n",
    "    matrix,\n",
    "    ['positive', 'negative', 'neutral'],\n",
    "    len(label_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metricas obtenidas usando RandomForest con 300 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=300, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfc.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de aciertos totales: 2954\n"
     ]
    }
   ],
   "source": [
    "matrix1 = confusion_matrix(label_test, predictions)\n",
    "aciertos = sum([matrix[i][i] for i in range(matrix.shape[0])])\n",
    "precision = aciertos / datos_totales_prueba\n",
    "print(\"Numero de aciertos totales: \" + str(aciertos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos para la prueba: 3836\n",
      "Precision promedio: 0.814256599182915\n",
      "Recall promedio: 0.8164783014679866\n",
      "\n",
      "Matriz de confusion: \n",
      "+----------+------------+------------+-----------+\n",
      "|          |   positive |   negative |   neutral |\n",
      "+==========+============+============+===========+\n",
      "| positive |        971 |         68 |       211 |\n",
      "+----------+------------+------------+-----------+\n",
      "| negative |         61 |       1142 |       129 |\n",
      "+----------+------------+------------+-----------+\n",
      "| neutral  |        134 |        106 |      1014 |\n",
      "+----------+------------+------------+-----------+\n",
      "\n",
      "Metricas de desempe単o: \n",
      "+----------+-------------+----------+\n",
      "|          |   precision |   recall |\n",
      "+==========+=============+==========+\n",
      "| positive |    0.7768   | 0.832762 |\n",
      "+----------+-------------+----------+\n",
      "| negative |    0.857357 | 0.867781 |\n",
      "+----------+-------------+----------+\n",
      "| neutral  |    0.808612 | 0.748892 |\n",
      "+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def imprimir_metricas_matriz_confusion_multiclase(matrix, headers, total_datos):\n",
    "    print(\"Numero de datos para la prueba: \" + str(total_datos))\n",
    "    metrics = np.zeros([len(headers), 2])\n",
    "    for i in range(matrix.shape[0]):\n",
    "        precision = matrix[i][i] / sum(matrix[i])\n",
    "        recoil = matrix[i][i] / sum([matrix[y][i] for y in range(matrix.shape[0])])\n",
    "        metrics[i][0] = precision\n",
    "        metrics[i][-1] = recoil\n",
    "    precision_promedio = sum([metrics[y][0] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    recall_promedio = sum([metrics[y][-1] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    print(\"Precision promedio: \" + str(precision_promedio))\n",
    "    print(\"Recall promedio: \" + str(recall_promedio))\n",
    "    print(\"\\nMatriz de confusion: \\n\" + str(tabulate(\n",
    "        matrix,\n",
    "        headers=headers,\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\\nMetricas de desempe単o: \\n\" + str(tabulate(\n",
    "        metrics,\n",
    "        headers=['precision', 'recall'],\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "imprimir_metricas_matriz_confusion_multiclase(\n",
    "    matrix1,\n",
    "    ['positive', 'negative', 'neutral'],\n",
    "    len(label_test)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
