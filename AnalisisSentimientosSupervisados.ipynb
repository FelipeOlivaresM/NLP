{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos supervisados para sentiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta primera sección del notebook corresponde con el método de cargar df, se usa para entrenar los diferentes modelos implícitamente ya que siempre que se entrena un modelo de identificacion de sentimientos se usa el mismo df, por lo que es indispensable para el proceso de entrenamiento, el método lo que hace es cargar de los diferentes archivos .csv reunidos por todos los grupos los datos tageados, los normaliza y dependiendo de los parámetros de entrada del método también los lematizar y los balancea, además de normalizar las etiquetas de sentimientos como 0 para sentimientos positivos, 1 para sentimientos negativos y 2 para sentimientos neutrales, en la parte de lematización es importante resaltar que se usan lematizadores diferentes para ingles y español.\n",
    "\n",
    "#### Carga de datos:\n",
    "Para la carga de datos el método lee la carpeta de datos para sentimientos, donde se encuentran enumerados los archivos .csv, dependiendo del número de archivo el método trata al archivo de una manera u otra, ya que los rangos de números corresponden con los diferentes equipos, siendo los datos de 1 de aerolineas, 2, 3 y 4 de un equipo, 5, 6 y 7 de otro y 8 del ultimo, por esto se tratan de formas distintas, en esta primera parte se cargan solo las columnas de texto, idioma y el tag de sentimientos, por lo que además se deben renombrar algunas columnas en ciertos archivos, luego de esto se eliminan las filas que tengan valores nulos en cualquier columna, se eliminan las duplicados usando la columna texto com indicador y se filtran los datos cargados de tal forma que solo pasen los datos que tienen como idiomas ingles o español y como tag 0, 1 o 2, además se reinician los índices del df, posterior a esto usando expresiones regulares se eliminan los saltos de línea los espaciados de más de un espacio, los url, y los espacios al principio y fin de cada texto en el df, esto para normalizar los datos entregados.\n",
    "\n",
    "#### Lematizacion:\n",
    "La lematización, que es indicada por la segunda variable de entrada del método, la cual es binaria, se vale de la columna de idioma o lang para lematizar el texto usando un lematizador para ingles o español, para esto recorre todo el df identificando el idioma con la columna lang y lematizando el texto uno a uno según el lematizador correspondiente.\n",
    "\n",
    "#### Balanceamiento de datos:\n",
    "Por último para el balanceamiento de los datos se usa la primera variable del método, la cual también es binaria, para balancear el df de salida, para esto se crea una columna adicional en el df llamada sello, en sello se usan los atributos de idioma y de mourning en conjunto para crear una marca en el df según estos dos atributos, luego de completar los sellos se usan estos para que todos los sellos queden con la misma cantidad de datos en el df usando un muestreo aleatorio en df que fueron agrupados y separados del resto según su sello o marca, la cantidad de datos en cada muestra se determina usando la cantidad del df más pequeño resultante de la separación y agrupación por sellos, luego se unen de nuevo los df de muestras y se elimina la columna sellos, de esta forma la cantidad de datos de idioma y mourning se encuentran iguales en el df de salida en cada posible combinación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feelings_df(balance_data, lematizacion):\n",
    "    from gensim.utils import any2unicode as unicode\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from sklearn.utils import resample\n",
    "    import os, re, sys, pandas, unidecode\n",
    "\n",
    "    feelings_folder = './entrenamiento de modelos/datos sentimientos'\n",
    "    feelings_df = pandas.DataFrame(columns=['text', 'lang', 'sentiment'])\n",
    "    path, subfolders, files_list = list(os.walk(feelings_folder))[0]\n",
    "    files_list.sort()\n",
    "\n",
    "    for i in range(len(files_list)):\n",
    "        sys.stdout.write(\"\\rPreparando df  \" + str(round(((i + 1) / (len(files_list))) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        file_name, file_ext = files_list[i].split(\".\")\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            file_path = path + \"/\" + file_name + \".\" + file_ext\n",
    "            df = pandas.read_csv(file_path, encoding='utf8', dtype=str, engine='python')\n",
    "            numero_de_archivo = int(file_name.split(\"_\")[0])\n",
    "\n",
    "            if numero_de_archivo == 1:\n",
    "                df = df.filter(['airline_sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 2 or numero_de_archivo == 3 or numero_de_archivo == 4:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 5 or numero_de_archivo == 6 or numero_de_archivo == 7:\n",
    "                df = df.filter(['polarity', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                if numero_de_archivo in [5, 6]: df['lang'] = 'es'\n",
    "                if numero_de_archivo == 7: df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 8:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'es'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "    del df\n",
    "    print(\"\")\n",
    "    feelings_df.dropna()\n",
    "    feelings_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    feelings_df = feelings_df.loc[feelings_df['sentiment'].isin(['0', '1', '2'])]\n",
    "    feelings_df = feelings_df.loc[feelings_df['lang'].isin(['es', 'en'])]\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i, row in feelings_df.iterrows():\n",
    "        sys.stdout.write(\"\\rNormalizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        feelings_df.at[i, 'text'] = (\n",
    "            re.sub(' +', ' ', re.sub(\"http\\S+\", \"\", re.sub('\\s+', ' ', str(feelings_df.at[i, 'text']))))\n",
    "        ).strip()\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    if balance_data == 1:\n",
    "        feelings_df.dropna()\n",
    "        feelings_df[\"Sello\"] = 0\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            #sys.stdout.write(\n",
    "             #   \"\\rCreando sellos de balanceamiento \" +\n",
    "             #   str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2))\n",
    "             #   + \"%\"\n",
    "            #)\n",
    "            #sys.stdout.flush()\n",
    "            feelings_df.at[i, 'sello'] = str(feelings_df.at[i, 'lang']) + '_' + str(feelings_df.at[i, 'sentiment'])\n",
    "        print(\"\\nBalanceando df\")\n",
    "        min_len1 = int(min(feelings_df['sello'].value_counts()))\n",
    "        df_0 = resample(feelings_df[feelings_df.sello == 'en_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_1 = resample(feelings_df[feelings_df.sello == 'en_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_2 = resample(feelings_df[feelings_df.sello == 'en_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_3 = resample(feelings_df[feelings_df.sello == 'es_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_4 = resample(feelings_df[feelings_df.sello == 'es_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_5 = resample(feelings_df[feelings_df.sello == 'es_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        feelings_df = pandas.concat([df_0, df_1, df_2, df_3, df_4, df_5])\n",
    "        feelings_df = feelings_df.filter(['text', 'lang', 'sentiment'])\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if lematizacion == 1:\n",
    "        stemmer_en = SnowballStemmer('english')\n",
    "        stemmer_es = SnowballStemmer('spanish')\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            sys.stdout.write(\"\\rLematizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "            sys.stdout.flush()\n",
    "            if type(feelings_df.at[i, 'text']) is str and feelings_df.at[i, 'lang'] == 'es':\n",
    "                feelings_df.at[i, 'text'] = str(stemmer_es.stem(feelings_df.at[i, 'text'])).lower()\n",
    "            elif type(feelings_df.at[i, 'text']) is str and feelings_df.at[i, 'lang'] == 'en':\n",
    "                feelings_df.at[i, 'text'] = str(stemmer_en.stem(feelings_df.at[i, 'text'])).lower()\n",
    "        print(\"\")\n",
    "\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"Df sentiments entregado\\n\")\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    data12 = pd.DataFrame(columns=('text', 'lang', 'sentiment','emoji'))\n",
    "    emoj=0\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "    for row in feelings_df.iterrows():\n",
    "        texto = row[1]['text']\n",
    "        text = re.sub(r'[\\b@]\\w+\\s{1}', '', texto)#Quitar menciones\n",
    "        words = tweet_tokenizer.tokenize(text)\n",
    "        for w in words:\n",
    "            if  w in emoji.UNICODE_EMOJI:\n",
    "                emoj = 1\n",
    "                break\n",
    "            else: emoj=0\n",
    "\n",
    "        lenguaje = row[1]['lang']\n",
    "        sentiment = row[1]['sentiment']\n",
    "        data12.loc[len(data12)]=[text,lenguaje,sentiment,emoj] \n",
    "        feelings_df = data12\n",
    "    \n",
    "    return feelings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 13.15%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 27.26%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 42.99%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 52.51%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 58.22%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 67.62%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 73.45%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 82.45%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 90.23%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 97.51%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando df 13.14%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando df 29.69%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando df 43.72%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando df 61.48%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando df 75.78%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando df 99.24%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_feelings_df(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets en el dataframe original: 15564\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTweets en el dataframe original: \" + str(df.shape[0]))\n",
    "df.columns.values\n",
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Lexicones\n",
    "#Lectura de SentiWordNet Obtenido de \n",
    "#https://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html\n",
    "\n",
    "# SentiWordNet[word] = {POS,\tID,\tPosScore,\tNegScore}\n",
    "contador = 0\n",
    "SentiWordNet = dict()\n",
    "for lines in open('./entrenamiento de modelos/data lexicon/SentiWordNet_3.0.0.txt'):\n",
    "    if lines.startswith('#'):\n",
    "        continue\n",
    "    line = lines.split('\\t')\n",
    "    palabra = line[4].split('#')[0]\n",
    "    if (palabra in SentiWordNet) or (palabra==''):\n",
    "        continue\n",
    "    else:\n",
    "        SentiWordNet[palabra]={'POS': line[0], 'ID': line[1], 'PosScore': line[2], 'NegScore': line[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFIN[word] = sentiment\n",
    "AFFIN = dict()\n",
    "for lines in open('./entrenamiento de modelos/data lexicon/AFFIN-111.txt'):\n",
    "    AFFIN[lines.split('\\t')[0]]=(lines.split('\\t')[1]).split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Esp Pos - Neg[word] = sentiment\n",
    "ESPpos = list()\n",
    "ESPneg = list()\n",
    "for lines in open('./entrenamiento de modelos/data lexicon/negative_words_es.txt'):\n",
    "    ESPneg.append(lines.split(\"\\n\")[0])\n",
    "for lines1 in open('./entrenamiento de modelos/data lexicon/positive_words_es.txt'):\n",
    "    ESPpos.append(lines1.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "def Lexicon(data1):\n",
    "    data = data1['text']\n",
    "    addlex = list()\n",
    "    i=0\n",
    "    for frase in data:\n",
    "        splited = tt.tokenize(frase)\n",
    "        \n",
    "        sum_swn_neg = 0\n",
    "        sum_swn_pos = 0\n",
    "        affin = 0\n",
    "        stnet = 0\n",
    "        contpos = 0\n",
    "        contneg = 0\n",
    "        word_stat = 0\n",
    "        total=0\n",
    "        for word in splited:\n",
    "        \n",
    "            #if word in SentiWordNet.keys():\n",
    "            #    sum_swn_neg += float(SentiWordNet[word]['NegScore'])\n",
    "            #    sum_swn_pos += float(SentiWordNet[word]['PosScore'])\n",
    "            #if word in AFFIN.keys():\n",
    "            #    affin += float(AFFIN[word])\n",
    "            if word in ESPpos:\n",
    "                contpos +=1\n",
    "            if word in ESPneg:\n",
    "                contneg +=1\n",
    "        total = contpos + contneg\n",
    "        if total!=0:\n",
    "            total = contpos / total\n",
    "        else: total=0.5\n",
    "        #datemo = int(data1['emoji'][i])\n",
    "        addlex.append([total])\n",
    "        i+=1\n",
    "    return addlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "def Lexicon1(data1):\n",
    "    data = data1['text']\n",
    "    addlex = list()\n",
    "    i=0\n",
    "    for frase in data:\n",
    "        splited = tt.tokenize(frase)\n",
    "        \n",
    "        sum_swn_neg = 0\n",
    "        sum_swn_pos = 0\n",
    "        affin = 0\n",
    "        stnet = 0\n",
    "        contpos = 0\n",
    "        contneg = 0\n",
    "        word_stat = 0\n",
    "        total=0\n",
    "        for word in splited:\n",
    "        \n",
    "            if word in SentiWordNet.keys():\n",
    "                sum_swn_neg += float(SentiWordNet[word]['NegScore'])\n",
    "                sum_swn_pos += float(SentiWordNet[word]['PosScore'])\n",
    "            if word in AFFIN.keys():\n",
    "                affin += float(AFFIN[word])\n",
    "            #if word in ESPpos:\n",
    "            #    contpos +=1\n",
    "            #if word in ESPneg:\n",
    "            #    contneg +=1\n",
    "        #total = contpos + contneg\n",
    "        #if total!=0:\n",
    "            #total = contpos / total\n",
    "        #else: total=0.5\n",
    "        datemo = int(data1['emoji'][i])\n",
    "        addlex.append([sum_swn_neg,sum_swn_pos,affin,datemo])\n",
    "        i+=1\n",
    "    return addlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extension(matriz, data):\n",
    "    extendLex = np.array(Lexicon(data))\n",
    "    return np.append(matriz, extendLex, 1)\n",
    "\n",
    "def extensionen(matriz, data):\n",
    "    extendLex = np.array(Lexicon1(data))\n",
    "    return np.append(matriz, extendLex, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=3000, min_df=7, max_df=0.8)\n",
    "\n",
    "# -------- Español.\n",
    "df_es = df[df1.lang == 'es']\n",
    "# -------- Ingles.\n",
    "df_en = df[df1.lang == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es.shape\n",
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(df_es['text']).toarray()\n",
    "featuresen = vectorizer.fit_transform(df_en['text']).toarray()\n",
    "\n",
    "labels = df_es['sentiment']\n",
    "labelsen = df_en['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape\n",
    "labelsen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 1696)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizacionLexicon = extension(features, df_es)\n",
    "vectorizacionLexicon.shape\n",
    "\n",
    "vectorizacionLexiconen = extensionen(featuresen, df_en)\n",
    "vectorizacionLexiconen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En esta segunda sección del notebook está el código necesario para entrenar los diferentes modelos supervisados de mourning dispuesto como método para ser implementado y usado fácilmente, este método se vale del de get_mourning_df, ya que este provee el df necesario para entrenar los diferentes modelos supervisados que se quieren entrenar.\n",
    "\n",
    "El pipeline seguido para entrenar los modelos de mourning se basa primero en seleccionar el modelo que se va a entrenar, este se selecciona con el primer parámetro del método, el cual es una cadena donde se escriben las iniciales del modelo: \n",
    "\n",
    "GBT: para gradient boosting trees.\n",
    "NN: para MPL o redes neuronales.\n",
    "DT: para árboles de decisión.\n",
    "RF: para random forest.\n",
    "NB: para naive bayes.\n",
    "\n",
    "Luego de seleccionar el modelo se carga el df haciendo uso del método get_mourning_df, los parámetros de este corresponden con los dos restantes del método de entrenamiento, luego de cargar el df este se divide según su idioma en 2 df, esto debido a que se entrenan modelos separados para ingles y español para integrar correctamente los lexicones posteriormente según el idioma, luego de esto se separan los df en los datos y etiquetas de prueba, este procedimiento también es independiente para cada idioma, posteriormente se vectorizan los datos tambien segun su idioma y se entrenan y prueban los modelos, en esta etapa se guardan los vocabularios de cada vectorizador aparte y también los modelos, esto con el fin de hacer uso de estos en otro programa posteriormente.\n",
    "\n",
    "Por último se muestran los resultados del modelo entrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelos_sentiments_supervisados(modelo_entr,vectorizacionLexicon,labels,vectorizacionLexicon1,labels1):\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    import pandas as pd\n",
    "    import pickle, os, nltk\n",
    "\n",
    "    modelos_es = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), n_estimators=10),\n",
    "        'RF': RandomForestClassifier(n_estimators=100, max_depth=300),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(50, 2), max_iter=100),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': BernoulliNB()\n",
    "    }\n",
    "    \n",
    "    modelos_en = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), n_estimators=10),\n",
    "        'RF': RandomForestClassifier(n_estimators=100, max_depth=300),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(50, 2), max_iter=100),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': BernoulliNB()\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # ---------------- Asignacion de los modelos y vectorizadores.\n",
    "    # -------- Español.\n",
    "    modelo_es = modelos_es[modelo_entr]\n",
    "\n",
    "    # -------- Ingles.\n",
    "    modelo_en = modelos_en[modelo_entr]\n",
    "        \n",
    "    \n",
    "    print(\"Separacion de datos por idioma terminada\")\n",
    "    # ---------------- Separacion en data y labels de entrenamiento.\n",
    "    # -------- Español.\n",
    "    data_train_es, data_test_es, label_train_es, label_test_es = train_test_split(\n",
    "        vectorizacionLexicon, labels, random_state=1\n",
    "    )\n",
    "    # -------- Ingles.\n",
    "    data_train_en, data_test_en, label_train_en, label_test_en = train_test_split(\n",
    "        vectorizacionLexicon1, labels1, random_state=1\n",
    "    )\n",
    "    \n",
    "    # ---------------- entrenamiento y guardado de los modelos.\n",
    "    # -------- Español.\n",
    "    ruta_modelo_es = './entrenamiento de modelos/modelos/' + str(type(modelo_es).__name__) + '_Sentiment_es.sav'\n",
    "    modelo_es.fit(data_train_es, label_train_es)\n",
    "    pickle.dump(modelo_es, open(ruta_modelo_es, 'wb'))\n",
    "    print(\"Modelo de \" + str(type(modelo_es).__name__) + \" en español guardado en \" + ruta_modelo_es)\n",
    "\n",
    "    # -------- Ingles.\n",
    "    ruta_modelo_en = './entrenamiento de modelos/modelos/' + str(type(modelo_en).__name__) + '_Sentiment_en.sav'\n",
    "    modelo_en.fit(data_train_en, label_train_en)\n",
    "    pickle.dump(modelo_en, open(ruta_modelo_en, 'wb'))\n",
    "    print(\"Modelo de \" + str(type(modelo_en).__name__) + \" en ingles guardado en \" + ruta_modelo_en)\n",
    "\n",
    "    # ---------------- implementacion de los modelos.\n",
    "    # -------- Español.\n",
    "    predictions_es = modelo_es.predict(data_test_es)\n",
    "    # -------- Ingles.\n",
    "    predictions_en = modelo_en.predict(data_test_en)\n",
    "    print(\"Predicciones terminadas\")\n",
    "    \n",
    "    \n",
    "    # ---------------- Resultados de los modelos.\n",
    "    # -------- Español.\n",
    "    print(\"\\nResultados \" + str(type(modelo_es).__name__) + \" Español:\\n\")\n",
    "    print(classification_report(label_test_es, predictions_es))\n",
    "    # -------- Ingles.\n",
    "    print(\"\\nResultados \" + str(type(modelo_en).__name__) + \" Ingles:\\n\")\n",
    "    print(classification_report(label_test_en, predictions_en))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separacion de datos por idioma terminada\n",
      "Modelo de BernoulliNB en español guardado en ./entrenamiento de modelos/modelos/BernoulliNB_Sentiment_es.sav\n",
      "Modelo de BernoulliNB en ingles guardado en ./entrenamiento de modelos/modelos/BernoulliNB_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados BernoulliNB Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       634\n",
      "           1       0.86      0.91      0.88       675\n",
      "           2       0.77      0.76      0.77       637\n",
      "\n",
      "    accuracy                           0.84      1946\n",
      "   macro avg       0.83      0.83      0.83      1946\n",
      "weighted avg       0.84      0.84      0.83      1946\n",
      "\n",
      "\n",
      "Resultados BernoulliNB Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75       634\n",
      "           1       0.79      0.78      0.78       675\n",
      "           2       0.69      0.73      0.71       637\n",
      "\n",
      "    accuracy                           0.75      1946\n",
      "   macro avg       0.75      0.75      0.75      1946\n",
      "weighted avg       0.75      0.75      0.75      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NB',vectorizacionLexicon,labels,vectorizacionLexiconen,labelsen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de DecisionTreeClassifier en español guardado en ./entrenamiento de modelos/modelos/DecisionTreeClassifier_Sentiment_es.sav\n",
      "Modelo de DecisionTreeClassifier en ingles guardado en ./entrenamiento de modelos/modelos/DecisionTreeClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados DecisionTreeClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.75      0.84       634\n",
      "           1       0.99      0.92      0.96       675\n",
      "           2       0.75      0.97      0.84       637\n",
      "\n",
      "    accuracy                           0.88      1946\n",
      "   macro avg       0.90      0.88      0.88      1946\n",
      "weighted avg       0.90      0.88      0.88      1946\n",
      "\n",
      "\n",
      "Resultados DecisionTreeClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.62      0.66       634\n",
      "           1       0.69      0.69      0.69       675\n",
      "           2       0.58      0.65      0.61       637\n",
      "\n",
      "    accuracy                           0.65      1946\n",
      "   macro avg       0.66      0.65      0.65      1946\n",
      "weighted avg       0.66      0.65      0.65      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('DT',vectorizacionLexicon,labels,vectorizacionLexiconen,labelsen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de RandomForestClassifier en español guardado en ./entrenamiento de modelos/modelos/RandomForestClassifier_Sentiment_es.sav\n",
      "Modelo de RandomForestClassifier en ingles guardado en ./entrenamiento de modelos/modelos/RandomForestClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados RandomForestClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.87       634\n",
      "           1       0.94      0.94      0.94       675\n",
      "           2       0.79      0.87      0.83       637\n",
      "\n",
      "    accuracy                           0.88      1946\n",
      "   macro avg       0.88      0.88      0.88      1946\n",
      "weighted avg       0.88      0.88      0.88      1946\n",
      "\n",
      "\n",
      "Resultados RandomForestClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       634\n",
      "           1       0.77      0.80      0.79       675\n",
      "           2       0.70      0.70      0.70       637\n",
      "\n",
      "    accuracy                           0.74      1946\n",
      "   macro avg       0.74      0.74      0.74      1946\n",
      "weighted avg       0.74      0.74      0.74      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('RF',vectorizacionLexicon,labels,vectorizacionLexiconen,labelsen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos GBT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de AdaBoostClassifier en español guardado en ./entrenamiento de modelos/modelos/AdaBoostClassifier_Sentiment_es.sav\n",
      "Modelo de AdaBoostClassifier en ingles guardado en ./entrenamiento de modelos/modelos/AdaBoostClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados AdaBoostClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85       634\n",
      "           1       0.98      0.93      0.95       675\n",
      "           2       0.78      0.91      0.84       637\n",
      "\n",
      "    accuracy                           0.88      1946\n",
      "   macro avg       0.89      0.88      0.88      1946\n",
      "weighted avg       0.89      0.88      0.88      1946\n",
      "\n",
      "\n",
      "Resultados AdaBoostClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67       634\n",
      "           1       0.72      0.69      0.70       675\n",
      "           2       0.58      0.61      0.60       637\n",
      "\n",
      "    accuracy                           0.66      1946\n",
      "   macro avg       0.66      0.65      0.66      1946\n",
      "weighted avg       0.66      0.66      0.66      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('GBT',vectorizacionLexicon,labels,vectorizacionLexiconen,labelsen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/felipedev/Documentos/entornopruebas/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de MLPClassifier en español guardado en ./entrenamiento de modelos/modelos/MLPClassifier_Sentiment_es.sav\n",
      "Modelo de MLPClassifier en ingles guardado en ./entrenamiento de modelos/modelos/MLPClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados MLPClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       634\n",
      "           1       0.83      0.88      0.85       675\n",
      "           2       0.79      0.67      0.72       637\n",
      "\n",
      "    accuracy                           0.80      1946\n",
      "   macro avg       0.80      0.80      0.80      1946\n",
      "weighted avg       0.80      0.80      0.80      1946\n",
      "\n",
      "\n",
      "Resultados MLPClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       634\n",
      "           1       0.75      0.70      0.72       675\n",
      "           2       0.62      0.66      0.64       637\n",
      "\n",
      "    accuracy                           0.69      1946\n",
      "   macro avg       0.69      0.69      0.69      1946\n",
      "weighted avg       0.69      0.69      0.69      1946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipedev/Documentos/entornopruebas/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NN',vectorizacionLexicon,labels,vectorizacionLexiconen,labelsen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
