{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos supervisados para sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta primera sección del notebook corresponde con el método de cargar df, se usa para entrenar los diferentes modelos implícitamente ya que siempre que se entrena un modelo de identificacion de sentimientos se usa el mismo df, por lo que es indispensable para el proceso de entrenamiento, el método lo que hace es cargar de los diferentes archivos .csv reunidos por todos los grupos los datos tageados, los normaliza y dependiendo de los parámetros de entrada del método también los lematizar y los balancea, además de normalizar las etiquetas de sentimientos como 0 para sentimientos positivos, 1 para sentimientos negativos y 2 para sentimientos neutrales, en la parte de lematización es importante resaltar que se usan lematizadores diferentes para ingles y español.\n",
    "\n",
    "#### Carga de datos:\n",
    "Para la carga de datos el método lee la carpeta de datos para sentimientos, donde se encuentran enumerados los archivos .csv, dependiendo del número de archivo el método trata al archivo de una manera u otra, ya que los rangos de números corresponden con los diferentes equipos, siendo los datos de 1 de aerolineas, 2, 3 y 4 de un equipo, 5, 6 y 7 de otro y 8 del ultimo, por esto se tratan de formas distintas, en esta primera parte se cargan solo las columnas de texto, idioma y el tag de sentimientos, por lo que además se deben renombrar algunas columnas en ciertos archivos, luego de esto se eliminan las filas que tengan valores nulos en cualquier columna, se eliminan las duplicados usando la columna texto com indicador y se filtran los datos cargados de tal forma que solo pasen los datos que tienen como idiomas ingles o español y como tag 0, 1 o 2, además se reinician los índices del df, posterior a esto usando expresiones regulares se eliminan los saltos de línea los espaciados de más de un espacio, los url, y los espacios al principio y fin de cada texto en el df, esto para normalizar los datos entregados.\n",
    "\n",
    "#### Lematizacion:\n",
    "La lematización, que es indicada por la segunda variable de entrada del método, la cual es binaria, se vale de la columna de idioma o lang para lematizar el texto usando un lematizador para ingles o español, para esto recorre todo el df identificando el idioma con la columna lang y lematizando el texto uno a uno según el lematizador correspondiente.\n",
    "\n",
    "#### Balanceamiento de datos:\n",
    "Por último para el balanceamiento de los datos se usa la primera variable del método, la cual también es binaria, para balancear el df de salida, para esto se crea una columna adicional en el df llamada sello, en sello se usan los atributos de idioma y de mourning en conjunto para crear una marca en el df según estos dos atributos, luego de completar los sellos se usan estos para que todos los sellos queden con la misma cantidad de datos en el df usando un muestreo aleatorio en df que fueron agrupados y separados del resto según su sello o marca, la cantidad de datos en cada muestra se determina usando la cantidad del df más pequeño resultante de la separación y agrupación por sellos, luego se unen de nuevo los df de muestras y se elimina la columna sellos, de esta forma la cantidad de datos de idioma y mourning se encuentran iguales en el df de salida en cada posible combinación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# balance_data: 0 para retornar dataframe sin balancear o 1 para retornar\n",
    "# dataframe balanceado.\n",
    "# lematizacion: lematiza los textos usando nltk 0 para no lematizar y 1\n",
    "# para retornar el texto lematizado.\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_feelings_df(balance_data, lematizacion):\n",
    "    from gensim.utils import any2unicode as unicode\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from sklearn.utils import resample\n",
    "    import os, re, sys, pandas, unidecode\n",
    "\n",
    "    feelings_folder = './entrenamiento de modelos/datos sentimientos'\n",
    "    feelings_df = pandas.DataFrame(columns=['text', 'lang', 'sentiment'])\n",
    "    path, subfolders, files_list = list(os.walk(feelings_folder))[0]\n",
    "    files_list.sort()\n",
    "\n",
    "    for i in range(len(files_list)):\n",
    "        sys.stdout.write(\"\\rPreparando df  \" + str(round(((i + 1) / (len(files_list))) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        file_name, file_ext = files_list[i].split(\".\")\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            file_path = path + \"/\" + file_name + \".\" + file_ext\n",
    "            df = pandas.read_csv(file_path, encoding='utf8', dtype=str, engine='python')\n",
    "            numero_de_archivo = int(file_name.split(\"_\")[0])\n",
    "\n",
    "            if numero_de_archivo == 1:\n",
    "                df = df.filter(['airline_sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 2 or numero_de_archivo == 3 or numero_de_archivo == 4:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 5 or numero_de_archivo == 6 or numero_de_archivo == 7:\n",
    "                df = df.filter(['polarity', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                if numero_de_archivo in [5, 6]: df['lang'] = 'es'\n",
    "                if numero_de_archivo == 7: df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 8:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'es'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "    del df\n",
    "    print(\"\")\n",
    "    feelings_df.dropna()\n",
    "    feelings_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    feelings_df = feelings_df.loc[feelings_df['sentiment'].isin(['0', '1', '2'])]\n",
    "    feelings_df = feelings_df.loc[feelings_df['lang'].isin(['es', 'en'])]\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i, row in feelings_df.iterrows():\n",
    "        sys.stdout.write(\"\\rNormalizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        feelings_df.at[i, 'text'] = (\n",
    "            re.sub('\\s+', ' ',\n",
    "                   re.sub(' +', ' ',\n",
    "                          re.sub(\"http\\S+\", \"\",\n",
    "                                 re.sub(r'\\b(?=\\w*[j])[aeiouj]{2,}\\b', 'jajaja',\n",
    "                                        re.sub(r'[\\b@]\\w+\\s{1}', '', str(feelings_df.at[i, 'text'])\n",
    "                                               )))))).strip()\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    if balance_data == 1:\n",
    "        feelings_df.dropna()\n",
    "        feelings_df[\"Sello\"] = 0\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            sys.stdout.write(\n",
    "                \"\\rCreando sellos de balanceamiento \" +\n",
    "                str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2))\n",
    "                + \"%\"\n",
    "            )\n",
    "            sys.stdout.flush()\n",
    "            feelings_df.at[i, 'sello'] = str(feelings_df.at[i, 'lang']) + '_' + str(feelings_df.at[i, 'sentiment'])\n",
    "        print(\"\\nBalanceando df\")\n",
    "        min_len1 = int(min(feelings_df['sello'].value_counts()))\n",
    "        df_0 = resample(feelings_df[feelings_df.sello == 'en_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_1 = resample(feelings_df[feelings_df.sello == 'en_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_2 = resample(feelings_df[feelings_df.sello == 'en_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_3 = resample(feelings_df[feelings_df.sello == 'es_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_4 = resample(feelings_df[feelings_df.sello == 'es_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_5 = resample(feelings_df[feelings_df.sello == 'es_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        feelings_df = pandas.concat([df_0, df_1, df_2, df_3, df_4, df_5])\n",
    "        feelings_df = feelings_df.filter(['text', 'lang', 'sentiment'])\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if lematizacion == 1:\n",
    "        stemmer_en = SnowballStemmer('english')\n",
    "        stemmer_es = SnowballStemmer('spanish')\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            sys.stdout.write(\"\\rLematizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "            sys.stdout.flush()\n",
    "            if type(feelings_df.at[i, 'text']) is str and feelings_df.at[i, 'lang'] == 'es':\n",
    "                feelings_df.at[i, 'text'] = stemmer_es.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "            elif type(feelings_df.at[i, 'text']) is str and feelings_df.at[i, 'lang'] == 'en':\n",
    "                feelings_df.at[i, 'text'] = stemmer_en.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Df sentiments entregado\\n\")\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "    return feelings_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta segunda sección del notebook está el código necesario para entrenar los diferentes modelos supervisados de mourning dispuesto como método para ser implementado y usado fácilmente, este método se vale del de get_mourning_df, ya que este provee el df necesario para entrenar los diferentes modelos supervisados que se quieren entrenar.\n",
    "\n",
    "El pipeline seguido para entrenar los modelos de mourning se basa primero en seleccionar el modelo que se va a entrenar, este se selecciona con el primer parámetro del método, el cual es una cadena donde se escriben las iniciales del modelo: \n",
    "\n",
    "GBT: para gradient boosting trees.\n",
    "NN: para MPL o redes neuronales.\n",
    "DT: para árboles de decisión.\n",
    "RF: para random forest.\n",
    "NB: para naive bayes.\n",
    "\n",
    "Luego de seleccionar el modelo se carga el df haciendo uso del método get_mourning_df, los parámetros de este corresponden con los dos restantes del método de entrenamiento, luego de cargar el df este se divide según su idioma en 2 df, esto debido a que se entrenan modelos separados para ingles y español para integrar correctamente los lexicones posteriormente según el idioma, luego de esto se separan los df en los datos y etiquetas de prueba, este procedimiento también es independiente para cada idioma, posteriormente se vectorizan los datos tambien segun su idioma y se entrenan y prueban los modelos, en esta etapa se guardan los vocabularios de cada vectorizador aparte y también los modelos, esto con el fin de hacer uso de estos en otro programa posteriormente.\n",
    "\n",
    "Por último se muestran los resultados del modelo entrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelos_sentiments_supervisados(modelo_entr, df_balanceado, df_lematizado):\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    import pandas as pd\n",
    "    import pickle, os, nltk\n",
    "\n",
    "    modelos_es = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=6), n_estimators=4),\n",
    "        'RF': RandomForestClassifier(n_estimators=14, max_depth=28),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(30, 1), max_iter=400),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': MultinomialNB()\n",
    "    }\n",
    "\n",
    "    modelos_en = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=6), n_estimators=4),\n",
    "        'RF': RandomForestClassifier(n_estimators=14, max_depth=32),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(40, 1), max_iter=400),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': MultinomialNB()\n",
    "    }\n",
    "\n",
    "    if modelo_entr in modelos_es and modelo_entr in modelos_en and 0 <= df_balanceado <= 1 and 0 <= df_lematizado <= 1:\n",
    "\n",
    "        print(\"\")\n",
    "        nltk.download('stopwords')\n",
    "\n",
    "        # ---------------- Asignacion de los modelos y vectorizadores.\n",
    "        # -------- Español.\n",
    "        modelo_es = modelos_es[modelo_entr]\n",
    "        vectorizer_es = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('spanish'))\n",
    "        # -------- Ingles.\n",
    "        modelo_en = modelos_en[modelo_entr]\n",
    "        vectorizer_en = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "        # ---------------- Lectura y separacion de datos.\n",
    "        df = pd.DataFrame(get_feelings_df(df_balanceado, df_lematizado))\n",
    "        # -------- Español.\n",
    "        df_es = df[df.lang == 'es']\n",
    "        # -------- Ingles.\n",
    "        df_en = df[df.lang == 'en']\n",
    "        del df\n",
    "        print(\"Separacion de datos por idioma terminada\")\n",
    "\n",
    "        # ---------------- Separacion en data y labels de entrenamiento.\n",
    "        # -------- Español.\n",
    "        data_train_es, data_test_es, label_train_es, label_test_es = train_test_split(\n",
    "            df_es['text'], df_es['sentiment'], random_state=1\n",
    "        )\n",
    "        del df_es\n",
    "        # -------- Ingles.\n",
    "        data_train_en, data_test_en, label_train_en, label_test_en = train_test_split(\n",
    "            df_en['text'], df_en['sentiment'], random_state=1\n",
    "        )\n",
    "        print(\"Division de datos terminada\")\n",
    "        del df_en\n",
    "\n",
    "        # ---------------- vectorizacion de los textos.\n",
    "        # -------- Español.\n",
    "        training_data_es = vectorizer_es.fit_transform(data_train_es)\n",
    "        testing_data_es = vectorizer_es.transform(data_test_es)\n",
    "        del data_train_es, data_test_es\n",
    "        print(\"Vectorizacion en español terminada\")\n",
    "        # -------- Ingles.\n",
    "        training_data_en = vectorizer_en.fit_transform(data_train_en)\n",
    "        testing_data_en = vectorizer_en.transform(data_test_en)\n",
    "        del data_test_en, data_train_en\n",
    "        print(\"Vectorizacion en ingles terminada\")\n",
    "\n",
    "        # ---------------- almacenamiento de los vocabularios.\n",
    "        # -------- Español.\n",
    "        ruta_vocabulario_es = \"./entrenamiento de modelos/vocabularios/vocabulario_sentiment_es.pkl\"\n",
    "        if os.path.exists(ruta_vocabulario_es):\n",
    "            os.remove(ruta_vocabulario_es)\n",
    "        pickle.dump(vectorizer_es.vocabulary_, open(ruta_vocabulario_es, \"wb\"))\n",
    "        print(\"Vocabulario para español almacenado en \" + ruta_vocabulario_es)\n",
    "        # -------- Ingles.\n",
    "        ruta_vocabulario_en = \"./entrenamiento de modelos/vocabularios/vocabulario_sentiment_en.pkl\"\n",
    "        if os.path.exists(ruta_vocabulario_en):\n",
    "            os.remove(ruta_vocabulario_en)\n",
    "        pickle.dump(vectorizer_en.vocabulary_, open(ruta_vocabulario_en, \"wb\"))\n",
    "        print(\"Vocabulario para ingles almacenado en \" + ruta_vocabulario_en)\n",
    "\n",
    "        # ---------------- entrenamiento y guardado de los modelos.\n",
    "        # -------- Español.\n",
    "        ruta_modelo_es = './entrenamiento de modelos/modelos/' + str(type(modelo_es).__name__) + '_Sentiment_es.sav'\n",
    "        modelo_es.fit(training_data_es, label_train_es)\n",
    "        pickle.dump(modelo_es, open(ruta_modelo_es, 'wb'))\n",
    "        print(\"Modelo de \" + str(type(modelo_es).__name__) + \" en español guardado en \" + ruta_modelo_es)\n",
    "        # -------- Ingles.\n",
    "        ruta_modelo_en = './entrenamiento de modelos/modelos/' + str(type(modelo_en).__name__) + '_Sentiment_en.sav'\n",
    "        modelo_en.fit(training_data_en, label_train_en)\n",
    "        pickle.dump(modelo_en, open(ruta_modelo_en, 'wb'))\n",
    "        print(\"Modelo de \" + str(type(modelo_en).__name__) + \" en ingles guardado en \" + ruta_modelo_en)\n",
    "\n",
    "        # ---------------- implementacion de los modelos.\n",
    "        # -------- Español.\n",
    "        predictions_es = modelo_es.predict(testing_data_es)\n",
    "        # -------- Ingles.\n",
    "        predictions_en = modelo_en.predict(testing_data_en)\n",
    "        print(\"Predicciones terminadas\")\n",
    "\n",
    "        # ---------------- Resultados de los modelos.\n",
    "        # -------- Español.\n",
    "        print(\"\\nResultados \" + str(type(modelo_es).__name__) + \" Español:\\n\")\n",
    "        print(classification_report(label_test_es, predictions_es))\n",
    "        # -------- Ingles.\n",
    "        print(\"\\nResultados \" + str(type(modelo_en).__name__) + \" Ingles:\\n\")\n",
    "        print(classification_report(label_test_en, predictions_en))\n",
    "\n",
    "    elif modelo_entr not in modelos_es or modelo_entr not in modelos_en or df_balanceado > 1 or df_balanceado < 0 or df_lematizado > 1 or df_lematizado < 0:\n",
    "\n",
    "        print(\"Parametros incorrectos para entrenar modelo\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando df  100.0%\n",
      "Creando sellos de balanceamiento 100.0%\n",
      "Balanceando df\n",
      "Lematizando df 97.99%"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NB',1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando sellos de balanceamiento 23.14%"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('DT',1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenar_modelos_sentiments_supervisados('RF',1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos GBT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenar_modelos_sentiments_supervisados('GBT',1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NN',1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
