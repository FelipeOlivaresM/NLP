{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Lectura y guardado de los documentos en un data frame de pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feelings_df(balance_data, lematizacion):\n",
    "    from gensim.utils import any2unicode as unicode\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from sklearn.utils import resample\n",
    "    import os, re, sys, pandas, unidecode\n",
    "\n",
    "    feelings_folder = './entrenamiento de modelos/datos sentimientos'\n",
    "    feelings_df = pandas.DataFrame(columns=['text', 'lang', 'sentiment'])\n",
    "    path, subfolders, files_list = list(os.walk(feelings_folder))[0]\n",
    "    files_list.sort()\n",
    "\n",
    "    for i in range(len(files_list)):\n",
    "        sys.stdout.write(\"\\rPreparando df  \" + str(round(((i + 1) / (len(files_list))) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        file_name, file_ext = files_list[i].split(\".\")\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            file_path = path + \"/\" + file_name + \".\" + file_ext\n",
    "            df = pandas.read_csv(file_path, encoding='utf8', dtype=str, engine='python')\n",
    "            numero_de_archivo = int(file_name.split(\"_\")[0])\n",
    "\n",
    "            if numero_de_archivo == 1:\n",
    "                df = df.filter(['airline_sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 2 or numero_de_archivo == 3 or numero_de_archivo == 4:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 5 or numero_de_archivo == 6 or numero_de_archivo == 7:\n",
    "                df = df.filter(['polarity', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                if numero_de_archivo in [5, 6]: df['lang'] = 'es'\n",
    "                if numero_de_archivo == 7: df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 8:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'es'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "    del df\n",
    "    print(\"\")\n",
    "    feelings_df.dropna()\n",
    "    feelings_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    feelings_df = feelings_df.loc[feelings_df['sentiment'].isin(['0', '1', '2'])]\n",
    "    feelings_df = feelings_df.loc[feelings_df['lang'].isin(['es', 'en'])]\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i, row in feelings_df.iterrows():\n",
    "        sys.stdout.write(\"\\rNormalizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        feelings_df.at[i, 'text'] = (\n",
    "            re.sub(' +', ' ', re.sub(\"http\\S+\", \"\", re.sub('\\s+', ' ', str(feelings_df.at[i, 'text']))))\n",
    "        ).strip()\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    if balance_data == 1:\n",
    "        feelings_df.dropna()\n",
    "        feelings_df[\"Sello\"] = 0\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            #sys.stdout.write(\n",
    "             #   \"\\rCreando sellos de balanceamiento \" +\n",
    "             #   str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2))\n",
    "             #   + \"%\"\n",
    "            #)\n",
    "            #sys.stdout.flush()\n",
    "            feelings_df.at[i, 'sello'] = str(feelings_df.at[i, 'lang']) + '_' + str(feelings_df.at[i, 'sentiment'])\n",
    "        print(\"\\nBalanceando df\")\n",
    "        min_len1 = int(min(feelings_df['sello'].value_counts()))\n",
    "        df_0 = resample(feelings_df[feelings_df.sello == 'en_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_1 = resample(feelings_df[feelings_df.sello == 'en_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_2 = resample(feelings_df[feelings_df.sello == 'en_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_3 = resample(feelings_df[feelings_df.sello == 'es_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_4 = resample(feelings_df[feelings_df.sello == 'es_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_5 = resample(feelings_df[feelings_df.sello == 'es_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        feelings_df = pandas.concat([df_0, df_1, df_2, df_3, df_4, df_5])\n",
    "        feelings_df = feelings_df.filter(['text', 'lang', 'sentiment'])\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if lematizacion == 1:\n",
    "        stemmer_en = SnowballStemmer('english')\n",
    "        stemmer_es = SnowballStemmer('spanish')\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            #sys.stdout.write(\"\\rLematizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "            #sys.stdout.flush()\n",
    "            if feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'es':\n",
    "                feelings_df.at[i, 'text'] = stemmer_es.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "            elif feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'en':\n",
    "                feelings_df.at[i, 'text'] = stemmer_en.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Df sentiments entregado\\n\")\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    data12 = pd.DataFrame(columns=('text', 'lang', 'sentiment','emoji'))\n",
    "    emoj=0\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "    for row in feelings_df.iterrows():\n",
    "        texto = row[1]['text']\n",
    "        text = re.sub(r'[\\b@]\\w+\\s{1}', '', texto)#Quitar menciones\n",
    "        words = tweet_tokenizer.tokenize(text)\n",
    "        for w in words:\n",
    "            if  w in emoji.UNICODE_EMOJI:\n",
    "                emoj = 1\n",
    "                break\n",
    "            else: emoj=0\n",
    "\n",
    "        lenguaje = row[1]['lang']\n",
    "        sentiment = row[1]['sentiment']\n",
    "        data12.loc[len(data12)]=[text,lenguaje,sentiment,emoj] \n",
    "        feelings_df = data12\n",
    "    \n",
    "    return feelings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando df  100.0%\n",
      "Normalizando df 100.0%\n",
      "\n",
      "Balanceando df\n",
      "\n",
      "Df sentiments entregado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_feelings_df(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets en el dataframe original: 15564\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTweets en el dataframe original: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['text', 'lang', 'sentiment', 'emoji'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Lexicones\n",
    "#Lectura de SentiWordNet Obtenido de \n",
    "#https://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html\n",
    "\n",
    "# SentiWordNet[word] = {POS,\tID,\tPosScore,\tNegScore}\n",
    "contador = 0\n",
    "SentiWordNet = dict()\n",
    "for lines in open('./entrenamiento de modelos/data lexicon/SentiWordNet_3.0.0.txt'):\n",
    "    if lines.startswith('#'):\n",
    "        continue\n",
    "    line = lines.split('\\t')\n",
    "    palabra = line[4].split('#')[0]\n",
    "    if (palabra in SentiWordNet) or (palabra==''):\n",
    "        continue\n",
    "    else:\n",
    "        SentiWordNet[palabra]={'POS': line[0], 'ID': line[1], 'PosScore': line[2], 'NegScore': line[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFIN[word] = sentiment\n",
    "AFFIN = dict()\n",
    "for lines in open('./entrenamiento de modelos/data lexicon/AFFIN-111.txt'):\n",
    "    AFFIN[lines.split('\\t')[0]]=(lines.split('\\t')[1]).split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Esp Pos - Neg[word] = sentiment\n",
    "ESPpos = list()\n",
    "ESPneg = list()\n",
    "for lines in open('./entrenamiento de modelos/data lexicon/negative_words_es.txt'):\n",
    "    ESPneg.append(lines.split(\"\\n\")[0])\n",
    "for lines1 in open('./entrenamiento de modelos/data lexicon/positive_words_es.txt'):\n",
    "    ESPpos.append(lines1.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "def Lexicon(data1):\n",
    "    data = data1['text']\n",
    "    addlex = list()\n",
    "    i=0\n",
    "    for frase in data:\n",
    "        splited = tt.tokenize(frase)\n",
    "        \n",
    "        sum_swn_neg = 0\n",
    "        sum_swn_pos = 0\n",
    "        affin = 0\n",
    "        stnet = 0\n",
    "        contpos = 0\n",
    "        contneg = 0\n",
    "        word_stat = 0\n",
    "        total=0\n",
    "        for word in splited:\n",
    "        \n",
    "            #if word in SentiWordNet.keys():\n",
    "            #    sum_swn_neg += float(SentiWordNet[word]['NegScore'])\n",
    "            #    sum_swn_pos += float(SentiWordNet[word]['PosScore'])\n",
    "            #if word in AFFIN.keys():\n",
    "            #    affin += float(AFFIN[word])\n",
    "            if word in ESPpos:\n",
    "                contpos +=1\n",
    "            if word in ESPneg:\n",
    "                contneg +=1\n",
    "        total = contpos + contneg\n",
    "        if total!=0:\n",
    "            total = contpos / total\n",
    "        else: total=0.5\n",
    "        #datemo = int(data1['emoji'][i])\n",
    "        addlex.append([total])\n",
    "        i+=1\n",
    "    return addlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extension(matriz, data):\n",
    "    extendLex = np.array(Lexicon(data))\n",
    "    return np.append(matriz, extendLex, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=3000, min_df=7, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Español.\n",
    "df_es = df[df1.lang == 'es']\n",
    "# -------- Ingles.\n",
    "df_en = df[df1.lang == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(df_es['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_es['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 2367)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizacionLexicon = extension(features, df_es)\n",
    "vectorizacionLexicon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in vectorizacionLexicon: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test = train_test_split(vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelos_sentiments_supervisados(modelo_entr,vectorizacionLexicon,labels):\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    import pandas as pd\n",
    "    import pickle, os, nltk\n",
    "\n",
    "    modelos_es = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), n_estimators=10),\n",
    "        'RF': RandomForestClassifier(n_estimators=100, max_depth=300),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(50, 2), max_iter=100),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    modelos_en = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), n_estimators=10),\n",
    "        'RF': RandomForestClassifier(n_estimators=100, max_depth=300),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(50, 2), max_iter=100),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': BernoulliNB()\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # ---------------- Asignacion de los modelos y vectorizadores.\n",
    "    # -------- Español.\n",
    "    modelo_es = modelos_es[modelo_entr]\n",
    "    #vectorizer_es = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('spanish'))\n",
    "    # -------- Ingles.\n",
    "    #modelo_en = modelos_en[modelo_entr]\n",
    "    #vectorizer_en = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "    # ---------------- Lectura y separacion de datos.\n",
    "    #df = pd.DataFrame(get_feelings_df(df_balanceado, df_lematizado))\n",
    "    # -------- Español.\n",
    "    #df_es = df[df.lang == 'es']\n",
    "    # -------- Ingles.\n",
    "    #df_en = df[df.lang == 'en']\n",
    "    #del df\n",
    "    print(\"Separacion de datos por idioma terminada\")\n",
    "\n",
    "    # ---------------- Separacion en data y labels de entrenamiento.\n",
    "    # -------- Español.\n",
    "    data_train_es, data_test_es, label_train_es, label_test_es = train_test_split(\n",
    "        vectorizacionLexicon, labels, random_state=1\n",
    "    )\n",
    "    #del df_es\n",
    "    # -------- Ingles.\n",
    "    #data_train_en, data_test_en, label_train_en, label_test_en = train_test_split(\n",
    "    #    vectorizacionLexicon, labels, random_state=1\n",
    "    #)\n",
    "    #print(\"Division de datos terminada\")\n",
    "    #del df_en\n",
    "\n",
    "    # ---------------- vectorizacion de los textos.\n",
    "    # -------- Español.\n",
    "    #training_data_es = vectorizer_es.fit_transform(data_train_es)\n",
    "    #testing_data_es = vectorizer_es.transform(data_test_es)\n",
    "    #del data_train_es, data_test_es\n",
    "    #print(\"Vectorizacion en español terminada\")\n",
    "    # -------- Ingles.\n",
    "    \n",
    "   # ---------------- entrenamiento y guardado de los modelos.\n",
    "    # -------- Español.\n",
    "    ruta_modelo_es = './entrenamiento de modelos/modelos/' + str(type(modelo_es).__name__) + '_Sentiment_es.sav'\n",
    "    modelo_es.fit(data_train_es, label_train_es)\n",
    "    pickle.dump(modelo_es, open(ruta_modelo_es, 'wb'))\n",
    "    print(\"Modelo de \" + str(type(modelo_es).__name__) + \" en español guardado en \" + ruta_modelo_es)\n",
    "\n",
    "    # -------- Ingles.\n",
    "    #ruta_modelo_en = './entrenamiento de modelos/modelos/' + str(type(modelo_en).__name__) + '_Sentiment_en.sav'\n",
    "    #modelo_en.fit(data_train_en, label_train_en)\n",
    "    #pickle.dump(modelo_en, open(ruta_modelo_en, 'wb'))\n",
    "    #print(\"Modelo de \" + str(type(modelo_en).__name__) + \" en ingles guardado en \" + ruta_modelo_en)\n",
    "\n",
    "    # ---------------- implementacion de los modelos.\n",
    "    # -------- Español.\n",
    "    predictions_es = modelo_es.predict(data_test_es)\n",
    "    # -------- Ingles.\n",
    "    #predictions_en = modelo_en.predict(data_test_en)\n",
    "    print(\"Predicciones terminadas\")\n",
    "    \n",
    "    \n",
    "    # ---------------- Resultados de los modelos.\n",
    "    # -------- Español.\n",
    "    print(\"\\nResultados \" + str(type(modelo_es).__name__) + \" Español:\\n\")\n",
    "    print(classification_report(label_test_es, predictions_es))\n",
    "    # -------- Ingles.\n",
    "    #print(\"\\nResultados \" + str(type(modelo_en).__name__) + \" Ingles:\\n\")\n",
    "    #print(classification_report(label_test_en, predictions_en))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separacion de datos por idioma terminada\n",
      "Modelo de GaussianNB en español guardado en ./entrenamiento de modelos/modelos/GaussianNB_Sentiment_es.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados GaussianNB Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.54      0.60       634\n",
      "           1       0.58      0.72      0.64       675\n",
      "           2       0.63      0.58      0.60       637\n",
      "\n",
      "    accuracy                           0.62      1946\n",
      "   macro avg       0.62      0.61      0.61      1946\n",
      "weighted avg       0.62      0.62      0.61      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NB',vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de DecisionTreeClassifier en español guardado en ./entrenamiento de modelos/modelos/DecisionTreeClassifier_Sentiment_es.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados DecisionTreeClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84       634\n",
      "           1       0.99      0.93      0.96       675\n",
      "           2       0.77      0.92      0.84       637\n",
      "\n",
      "    accuracy                           0.88      1946\n",
      "   macro avg       0.89      0.88      0.88      1946\n",
      "weighted avg       0.89      0.88      0.88      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('DT',vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de RandomForestClassifier en español guardado en ./entrenamiento de modelos/modelos/RandomForestClassifier_Sentiment_es.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados RandomForestClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.81      0.86       634\n",
      "           1       0.96      0.93      0.95       675\n",
      "           2       0.78      0.89      0.83       637\n",
      "\n",
      "    accuracy                           0.88      1946\n",
      "   macro avg       0.89      0.88      0.88      1946\n",
      "weighted avg       0.89      0.88      0.88      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('RF',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos GBT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de AdaBoostClassifier en español guardado en ./entrenamiento de modelos/modelos/AdaBoostClassifier_Sentiment_es.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados AdaBoostClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       634\n",
      "           1       0.95      0.94      0.94       675\n",
      "           2       0.79      0.86      0.82       637\n",
      "\n",
      "    accuracy                           0.87      1946\n",
      "   macro avg       0.87      0.87      0.87      1946\n",
      "weighted avg       0.87      0.87      0.87      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('GBT',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de MLPClassifier en español guardado en ./entrenamiento de modelos/modelos/MLPClassifier_Sentiment_es.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados MLPClassifier Español:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       634\n",
      "           1       0.00      0.00      0.00       675\n",
      "           2       0.33      1.00      0.49       637\n",
      "\n",
      "    accuracy                           0.33      1946\n",
      "   macro avg       0.11      0.33      0.16      1946\n",
      "weighted avg       0.11      0.33      0.16      1946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/Documentos/PytonVenv/venv_nlp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NN',vectorizacionLexicon,labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
