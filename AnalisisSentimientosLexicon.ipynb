{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Lectura y guardado de los documentos en un data frame de pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feelings_df(balance_data, lematizacion):\n",
    "    from gensim.utils import any2unicode as unicode\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from sklearn.utils import resample\n",
    "    import os, re, sys, pandas, unidecode\n",
    "\n",
    "    feelings_folder = './entrenamiento de modelos/datos sentimientos'\n",
    "    feelings_df = pandas.DataFrame(columns=['text', 'lang', 'sentiment'])\n",
    "    path, subfolders, files_list = list(os.walk(feelings_folder))[0]\n",
    "    files_list.sort()\n",
    "\n",
    "    for i in range(len(files_list)):\n",
    "        sys.stdout.write(\"\\rPreparando df  \" + str(round(((i + 1) / (len(files_list))) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        file_name, file_ext = files_list[i].split(\".\")\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            file_path = path + \"/\" + file_name + \".\" + file_ext\n",
    "            df = pandas.read_csv(file_path, encoding='utf8', dtype=str, engine='python')\n",
    "            numero_de_archivo = int(file_name.split(\"_\")[0])\n",
    "\n",
    "            if numero_de_archivo == 1:\n",
    "                df = df.filter(['airline_sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 2 or numero_de_archivo == 3 or numero_de_archivo == 4:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 5 or numero_de_archivo == 6 or numero_de_archivo == 7:\n",
    "                df = df.filter(['polarity', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                if numero_de_archivo in [5, 6]: df['lang'] = 'es'\n",
    "                if numero_de_archivo == 7: df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 8:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'es'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "    del df\n",
    "    print(\"\")\n",
    "    feelings_df.dropna()\n",
    "    feelings_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    feelings_df = feelings_df.loc[feelings_df['sentiment'].isin(['0', '1', '2'])]\n",
    "    feelings_df = feelings_df.loc[feelings_df['lang'].isin(['es', 'en'])]\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i, row in feelings_df.iterrows():\n",
    "        sys.stdout.write(\"\\rNormalizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        feelings_df.at[i, 'text'] = (\n",
    "            re.sub(' +', ' ', re.sub(\"http\\S+\", \"\", re.sub('\\s+', ' ', str(feelings_df.at[i, 'text']))))\n",
    "        ).strip()\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    if balance_data == 1:\n",
    "        feelings_df.dropna()\n",
    "        feelings_df[\"Sello\"] = 0\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            #sys.stdout.write(\n",
    "             #   \"\\rCreando sellos de balanceamiento \" +\n",
    "             #   str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2))\n",
    "             #   + \"%\"\n",
    "            #)\n",
    "            #sys.stdout.flush()\n",
    "            feelings_df.at[i, 'sello'] = str(feelings_df.at[i, 'lang']) + '_' + str(feelings_df.at[i, 'sentiment'])\n",
    "        print(\"\\nBalanceando df\")\n",
    "        min_len1 = int(min(feelings_df['sello'].value_counts()))\n",
    "        df_0 = resample(feelings_df[feelings_df.sello == 'en_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_1 = resample(feelings_df[feelings_df.sello == 'en_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_2 = resample(feelings_df[feelings_df.sello == 'en_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_3 = resample(feelings_df[feelings_df.sello == 'es_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_4 = resample(feelings_df[feelings_df.sello == 'es_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_5 = resample(feelings_df[feelings_df.sello == 'es_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        feelings_df = pandas.concat([df_0, df_1, df_2, df_3, df_4, df_5])\n",
    "        feelings_df = feelings_df.filter(['text', 'lang', 'sentiment'])\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if lematizacion == 1:\n",
    "        stemmer_en = SnowballStemmer('english')\n",
    "        stemmer_es = SnowballStemmer('spanish')\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            #sys.stdout.write(\"\\rLematizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "            #sys.stdout.flush()\n",
    "            if feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'es':\n",
    "                feelings_df.at[i, 'text'] = stemmer_es.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "            elif feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'en':\n",
    "                feelings_df.at[i, 'text'] = stemmer_en.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Df sentiments entregado\\n\")\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "    return feelings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando df  100.0%\n",
      "Normalizando df 0.16%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipedev/Documentos/entornopruebas/lib/python3.6/site-packages/pandas/core/frame.py:6701: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 9.22%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 24.46%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 39.38%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 54.41%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 69.56%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 84.72%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 100.0%\n",
      "\n",
      "Balanceando df\n",
      "\n",
      "Df sentiments entregado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_feelings_df(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets en el dataframe original: 15564\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTweets en el dataframe original: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>PERIODISMO IRRESPONSABLE Van apareciendo los v...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>Mañana miércoles 6 de mayo, en Autores en cuar...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15561</th>\n",
       "      <td>COVID-19 Desescalada comienzo Fase 1 el 11 de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>Coronavirus: nuevo comunicado del Comité de Em...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15563</th>\n",
       "      <td>che en gba no están para flexibilizar nada de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang sentiment\n",
       "15559  PERIODISMO IRRESPONSABLE Van apareciendo los v...   es         2\n",
       "15560  Mañana miércoles 6 de mayo, en Autores en cuar...   es         2\n",
       "15561  COVID-19 Desescalada comienzo Fase 1 el 11 de ...   es         2\n",
       "15562  Coronavirus: nuevo comunicado del Comité de Em...   es         2\n",
       "15563  che en gba no están para flexibilizar nada de ...   es         2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir looking forward to the beats mus...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanks to @theBeardedCripl and @hannietravels ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@SouthwestAir great cabin and flight crew this...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus and Communities: Activists step up...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@JetBlue Thanks. Still booked our trip 3/13-17...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@JetBlue gr8 #Mint crew on #flight 123 to #LAX...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@SouthwestAir Thank you for the tip!</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@southwestair SWEET!!! Glad to hear it. I'll k...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Happy #TT to my friends @AmericanAir . Hope th...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@united Thank you. Took care of everything and...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@united despite shaky connections, looks like ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@SouthwestAir Yeah, we figured it out. Thanks.</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@SouthwestAir thankyou :))❤️</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@JetBlue thanks for the response - when is the...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@AmericanAir thanks, me too</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@SouthwestAir give this guy a raise....great s...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Adapting Small Business to COVID-19 FourTen Cr...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@JetBlue thanks for the response. We are hopeful.</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@JetBlue @CinziannaP thank you! I like the qui...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@SouthwestAir CEO Gary Kelly, \"We are America'...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@JetBlue ha ha! Can I get a wake up call at bo...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@united great flight into PVD. Smallest plane ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@united thank you! Love united!! Have 4 flight...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@VirginAmerica Keep up the great work :)</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@JetBlue Really!? That's good to hear! Thanks ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@SouthwestAir happy to enter your sweepstakes ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@united we got it, thanks.</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@SouthwestAir @AmericanAir y'all are better th...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@VirginAmerica has getaway deals through May, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15534</th>\n",
       "      <td>A partir de qué día de la cuarentena pasa a se...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>Ya quiero que se termine esta cuarentena para ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>Como hacen para sacarse fotos lindas en cuaren...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15537</th>\n",
       "      <td>Un señor acaba de comprar sus tamales de yapa ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15538</th>\n",
       "      <td>A mí no me importa engordar en la cuarentena ,...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15539</th>\n",
       "      <td>Ya que pase la cuarentena</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15540</th>\n",
       "      <td>Por tercera ocasión, las autoridades de Ecuado...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15541</th>\n",
       "      <td>\"Coronavirus: Librerías, jugueterías y otros c...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15542</th>\n",
       "      <td>Deben leerlo aunque se que algunos no lo enten...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15543</th>\n",
       "      <td>🔴Mapa de contagios #Covid19_mx en #Cuernavaca🔴...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15544</th>\n",
       "      <td>Termina esta cuarentena y por mi paz mental qu...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15545</th>\n",
       "      <td>Hoy precisamente que entramos en cuarentena es...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15546</th>\n",
       "      <td>¡Es el Martes! Y a como va este 2020 es probab...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>Toda la cuarentena e usado short, ¿aún sabre u...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15548</th>\n",
       "      <td>Nos fumamos como 2 meses con el español y just...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>Leo \"webinar\", \"webinar\"! En todo lado y juro ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>Cuándo se termina la cuarentena, I'm harta de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>La letalidad del Covid en México al 9 de mayo:...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>La pandemia del coronavirus hace más necesario...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>No se ustedes pero yo terminando la cuarentena...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554</th>\n",
       "      <td>Le tengo más miedo a los políticos que al coro...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15555</th>\n",
       "      <td>Pa’ como vamos el #coronavirus también va a ar...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556</th>\n",
       "      <td>HAY QUE DESTERRAR ESOS ACTOS COMO SI FUERA EL ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15557</th>\n",
       "      <td>@Claudiashein Los brotes de covid que surjan d...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15558</th>\n",
       "      <td>¡VUELVE LA JOYA 💎 ⚽️! El atacante argentino Pa...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>PERIODISMO IRRESPONSABLE Van apareciendo los v...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>Mañana miércoles 6 de mayo, en Autores en cuar...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15561</th>\n",
       "      <td>COVID-19 Desescalada comienzo Fase 1 el 11 de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>Coronavirus: nuevo comunicado del Comité de Em...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15563</th>\n",
       "      <td>che en gba no están para flexibilizar nada de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15564 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang sentiment\n",
       "0      @SouthwestAir looking forward to the beats mus...   en         0\n",
       "1      Thanks to @theBeardedCripl and @hannietravels ...   en         0\n",
       "2      @SouthwestAir great cabin and flight crew this...   en         0\n",
       "3      Coronavirus and Communities: Activists step up...   en         0\n",
       "4      @JetBlue Thanks. Still booked our trip 3/13-17...   en         0\n",
       "5      @JetBlue gr8 #Mint crew on #flight 123 to #LAX...   en         0\n",
       "6                   @SouthwestAir Thank you for the tip!   en         0\n",
       "7      @southwestair SWEET!!! Glad to hear it. I'll k...   en         0\n",
       "8      Happy #TT to my friends @AmericanAir . Hope th...   en         0\n",
       "9      @united Thank you. Took care of everything and...   en         0\n",
       "10     @united despite shaky connections, looks like ...   en         0\n",
       "11        @SouthwestAir Yeah, we figured it out. Thanks.   en         0\n",
       "12                          @SouthwestAir thankyou :))❤️   en         0\n",
       "13     @JetBlue thanks for the response - when is the...   en         0\n",
       "14                           @AmericanAir thanks, me too   en         0\n",
       "15     @SouthwestAir give this guy a raise....great s...   en         0\n",
       "16     Adapting Small Business to COVID-19 FourTen Cr...   en         0\n",
       "17     @JetBlue thanks for the response. We are hopeful.   en         0\n",
       "18     @JetBlue @CinziannaP thank you! I like the qui...   en         0\n",
       "19     @SouthwestAir CEO Gary Kelly, \"We are America'...   en         0\n",
       "20     @VirginAmerica @virginmedia I'm flying your #f...   en         0\n",
       "21     @JetBlue ha ha! Can I get a wake up call at bo...   en         0\n",
       "22     @united great flight into PVD. Smallest plane ...   en         0\n",
       "23     @united thank you! Love united!! Have 4 flight...   en         0\n",
       "24              @VirginAmerica Keep up the great work :)   en         0\n",
       "25     @JetBlue Really!? That's good to hear! Thanks ...   en         0\n",
       "26     @SouthwestAir happy to enter your sweepstakes ...   en         0\n",
       "27                            @united we got it, thanks.   en         0\n",
       "28     @SouthwestAir @AmericanAir y'all are better th...   en         0\n",
       "29     @VirginAmerica has getaway deals through May, ...   en         0\n",
       "...                                                  ...  ...       ...\n",
       "15534  A partir de qué día de la cuarentena pasa a se...   es         2\n",
       "15535  Ya quiero que se termine esta cuarentena para ...   es         2\n",
       "15536  Como hacen para sacarse fotos lindas en cuaren...   es         2\n",
       "15537  Un señor acaba de comprar sus tamales de yapa ...   es         2\n",
       "15538  A mí no me importa engordar en la cuarentena ,...   es         2\n",
       "15539                          Ya que pase la cuarentena   es         2\n",
       "15540  Por tercera ocasión, las autoridades de Ecuado...   es         2\n",
       "15541  \"Coronavirus: Librerías, jugueterías y otros c...   es         2\n",
       "15542  Deben leerlo aunque se que algunos no lo enten...   es         2\n",
       "15543  🔴Mapa de contagios #Covid19_mx en #Cuernavaca🔴...   es         2\n",
       "15544  Termina esta cuarentena y por mi paz mental qu...   es         2\n",
       "15545  Hoy precisamente que entramos en cuarentena es...   es         2\n",
       "15546  ¡Es el Martes! Y a como va este 2020 es probab...   es         2\n",
       "15547  Toda la cuarentena e usado short, ¿aún sabre u...   es         2\n",
       "15548  Nos fumamos como 2 meses con el español y just...   es         2\n",
       "15549  Leo \"webinar\", \"webinar\"! En todo lado y juro ...   es         2\n",
       "15550  Cuándo se termina la cuarentena, I'm harta de ...   es         2\n",
       "15551  La letalidad del Covid en México al 9 de mayo:...   es         2\n",
       "15552  La pandemia del coronavirus hace más necesario...   es         2\n",
       "15553  No se ustedes pero yo terminando la cuarentena...   es         2\n",
       "15554  Le tengo más miedo a los políticos que al coro...   es         2\n",
       "15555  Pa’ como vamos el #coronavirus también va a ar...   es         2\n",
       "15556  HAY QUE DESTERRAR ESOS ACTOS COMO SI FUERA EL ...   es         2\n",
       "15557  @Claudiashein Los brotes de covid que surjan d...   es         2\n",
       "15558  ¡VUELVE LA JOYA 💎 ⚽️! El atacante argentino Pa...   es         2\n",
       "15559  PERIODISMO IRRESPONSABLE Van apareciendo los v...   es         2\n",
       "15560  Mañana miércoles 6 de mayo, en Autores en cuar...   es         2\n",
       "15561  COVID-19 Desescalada comienzo Fase 1 el 11 de ...   es         2\n",
       "15562  Coronavirus: nuevo comunicado del Comité de Em...   es         2\n",
       "15563  che en gba no están para flexibilizar nada de ...   es         2\n",
       "\n",
       "[15564 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Lexicones\n",
    "#Lectura de SentiWordNet Obtenido de \n",
    "#https://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html\n",
    "\n",
    "# SentiWordNet[word] = {POS,\tID,\tPosScore,\tNegScore}\n",
    "contador = 0\n",
    "SentiWordNet = dict()\n",
    "for lines in open('Data Lexicon/SentiWordNet_3.0.0.txt'):\n",
    "    if lines.startswith('#'):\n",
    "        continue\n",
    "    line = lines.split('\\t')\n",
    "    palabra = line[4].split('#')[0]\n",
    "    if (palabra in SentiWordNet) or (palabra==''):\n",
    "        continue\n",
    "    else:\n",
    "        SentiWordNet[palabra]={'POS': line[0], 'ID': line[1], 'PosScore': line[2], 'NegScore': line[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFIN[word] = sentiment\n",
    "AFFIN = dict()\n",
    "for lines in open('Data Lexicon/AFFIN-111.txt'):\n",
    "    AFFIN[lines.split('\\t')[0]]=(lines.split('\\t')[1]).split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "def Lexicon(data):\n",
    "    addlex = list()\n",
    "    for frase in data:\n",
    "        splited = tt.tokenize(frase)\n",
    "        sum_swn_neg = 0\n",
    "        sum_swn_pos = 0\n",
    "        affin = 0\n",
    "        stnet = 0\n",
    "        word_stat = 0\n",
    "        for word in splited:\n",
    "            if word in SentiWordNet.keys():\n",
    "                sum_swn_neg += float(SentiWordNet[word]['NegScore'])\n",
    "                sum_swn_pos += float(SentiWordNet[word]['PosScore'])\n",
    "            if word in AFFIN.keys():\n",
    "                affin += float(AFFIN[word])\n",
    "            \n",
    "        addlex.append([sum_swn_neg, sum_swn_pos, affin])\n",
    "    return addlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extension(matriz, data):\n",
    "    extendLex = np.array(Lexicon(data))\n",
    "    return np.append(matriz, extendLex, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=3000, min_df=7, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Español.\n",
    "df_es = df[df1.lang == 'es']\n",
    "# -------- Ingles.\n",
    "df_en = df[df1.lang == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(df_en['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_en['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizacionLexicon = extension(features, df_en['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in vectorizacionLexicon: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test = train_test_split(vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelos_sentiments_supervisados(modelo_entr,vectorizacionLexicon,labels):\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    import pandas as pd\n",
    "    import pickle, os, nltk\n",
    "\n",
    "\n",
    "    modelos_en = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), n_estimators=10),\n",
    "        'RF': RandomForestClassifier(n_estimators=100, max_depth=300),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(50, 2), max_iter=100),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': GaussianNB()\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # ---------------- Asignacion de los modelos y vectorizadores.\n",
    "    # -------- Español.\n",
    "    #modelo_es = modelos_es[modelo_entr]\n",
    "    #vectorizer_es = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('spanish'))\n",
    "    # -------- Ingles.\n",
    "    modelo_en = modelos_en[modelo_entr]\n",
    "    #vectorizer_en = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "    # ---------------- Lectura y separacion de datos.\n",
    "    #df = pd.DataFrame(get_feelings_df(df_balanceado, df_lematizado))\n",
    "    # -------- Español.\n",
    "    #df_es = df[df.lang == 'es']\n",
    "    # -------- Ingles.\n",
    "    #df_en = df[df.lang == 'en']\n",
    "    #del df\n",
    "    print(\"Separacion de datos por idioma terminada\")\n",
    "\n",
    "    # ---------------- Separacion en data y labels de entrenamiento.\n",
    "    # -------- Español.\n",
    "    #data_train_es, data_test_es, label_train_es, label_test_es = train_test_split(\n",
    "    #    df_es['text'], df_es['sentiment'], random_state=1\n",
    "    #)\n",
    "    #del df_es\n",
    "    # -------- Ingles.\n",
    "    data_train_en, data_test_en, label_train_en, label_test_en = train_test_split(\n",
    "        vectorizacionLexicon, labels, random_state=1\n",
    "    )\n",
    "    #print(\"Division de datos terminada\")\n",
    "    #del df_en\n",
    "\n",
    "    # ---------------- vectorizacion de los textos.\n",
    "    # -------- Español.\n",
    "    #training_data_es = vectorizer_es.fit_transform(data_train_es)\n",
    "    #testing_data_es = vectorizer_es.transform(data_test_es)\n",
    "    #del data_train_es, data_test_es\n",
    "    #print(\"Vectorizacion en español terminada\")\n",
    "    # -------- Ingles.\n",
    "    \n",
    "\n",
    "\n",
    "    # -------- Ingles.\n",
    "    ruta_modelo_en = './entrenamiento de modelos/modelos/' + str(type(modelo_en).__name__) + '_Sentiment_en.sav'\n",
    "    modelo_en.fit(data_train_en, label_train_en)\n",
    "    pickle.dump(modelo_en, open(ruta_modelo_en, 'wb'))\n",
    "    print(\"Modelo de \" + str(type(modelo_en).__name__) + \" en ingles guardado en \" + ruta_modelo_en)\n",
    "\n",
    "\n",
    "    # -------- Ingles.\n",
    "    predictions_en = modelo_en.predict(data_test_en)\n",
    "    print(\"Predicciones terminadas\")\n",
    "\n",
    "    # -------- Ingles.\n",
    "    print(\"\\nResultados \" + str(type(modelo_en).__name__) + \" Ingles:\\n\")\n",
    "    print(classification_report(label_test_en, predictions_en))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n",
      "Modelo de GaussianNB en ingles guardado en ./entrenamiento de modelos/modelos/GaussianNB_Sentiment_en.sav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones terminadas\n",
      "\n",
      "Resultados GaussianNB Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.70      0.61       634\n",
      "           1       0.67      0.70      0.68       675\n",
      "           2       0.61      0.41      0.49       637\n",
      "\n",
      "    accuracy                           0.60      1946\n",
      "   macro avg       0.61      0.60      0.60      1946\n",
      "weighted avg       0.61      0.60      0.60      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NB',vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de DecisionTreeClassifier en ingles guardado en ./entrenamiento de modelos/modelos/DecisionTreeClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados DecisionTreeClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       634\n",
      "           1       0.67      0.74      0.71       675\n",
      "           2       0.64      0.59      0.62       637\n",
      "\n",
      "    accuracy                           0.66      1946\n",
      "   macro avg       0.66      0.66      0.66      1946\n",
      "weighted avg       0.66      0.66      0.66      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('DT',vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de RandomForestClassifier en ingles guardado en ./entrenamiento de modelos/modelos/RandomForestClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados RandomForestClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74       634\n",
      "           1       0.76      0.82      0.79       675\n",
      "           2       0.71      0.71      0.71       637\n",
      "\n",
      "    accuracy                           0.75      1946\n",
      "   macro avg       0.75      0.75      0.75      1946\n",
      "weighted avg       0.75      0.75      0.75      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('RF',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos GBT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de AdaBoostClassifier en ingles guardado en ./entrenamiento de modelos/modelos/AdaBoostClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados AdaBoostClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       634\n",
      "           1       0.69      0.70      0.69       675\n",
      "           2       0.62      0.60      0.61       637\n",
      "\n",
      "    accuracy                           0.66      1946\n",
      "   macro avg       0.65      0.66      0.66      1946\n",
      "weighted avg       0.66      0.66      0.66      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('GBT',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de MLPClassifier en ingles guardado en ./entrenamiento de modelos/modelos/MLPClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados MLPClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       634\n",
      "           1       0.77      0.67      0.72       675\n",
      "           2       0.62      0.69      0.65       637\n",
      "\n",
      "    accuracy                           0.69      1946\n",
      "   macro avg       0.69      0.69      0.69      1946\n",
      "weighted avg       0.69      0.69      0.69      1946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipedev/Documentos/entornopruebas/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NN',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
