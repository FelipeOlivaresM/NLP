{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Lectura y guardado de los documentos en un data frame de pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feelings_df(balance_data, lematizacion):\n",
    "    from gensim.utils import any2unicode as unicode\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from sklearn.utils import resample\n",
    "    import os, re, sys, pandas, unidecode\n",
    "\n",
    "    feelings_folder = './entrenamiento de modelos/datos sentimientos'\n",
    "    feelings_df = pandas.DataFrame(columns=['text', 'lang', 'sentiment'])\n",
    "    path, subfolders, files_list = list(os.walk(feelings_folder))[0]\n",
    "    files_list.sort()\n",
    "\n",
    "    for i in range(len(files_list)):\n",
    "        sys.stdout.write(\"\\rPreparando df  \" + str(round(((i + 1) / (len(files_list))) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        file_name, file_ext = files_list[i].split(\".\")\n",
    "\n",
    "        if file_ext == 'csv':\n",
    "            file_path = path + \"/\" + file_name + \".\" + file_ext\n",
    "            df = pandas.read_csv(file_path, encoding='utf8', dtype=str, engine='python')\n",
    "            numero_de_archivo = int(file_name.split(\"_\")[0])\n",
    "\n",
    "            if numero_de_archivo == 1:\n",
    "                df = df.filter(['airline_sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 2 or numero_de_archivo == 3 or numero_de_archivo == 4:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 5 or numero_de_archivo == 6 or numero_de_archivo == 7:\n",
    "                df = df.filter(['polarity', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                if numero_de_archivo in [5, 6]: df['lang'] = 'es'\n",
    "                if numero_de_archivo == 7: df['lang'] = 'en'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "            if numero_de_archivo == 8:\n",
    "                df = df.filter(['sentiment', 'text'])\n",
    "                df.columns = ['sentiment', 'text']\n",
    "                df['lang'] = 'es'\n",
    "                df['sentiment'] = df.sentiment.map({'positive': '0', 'negative': '1', 'neutral': '2'})\n",
    "                feelings_df = feelings_df.append(df)\n",
    "\n",
    "    del df\n",
    "    print(\"\")\n",
    "    feelings_df.dropna()\n",
    "    feelings_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    feelings_df = feelings_df.loc[feelings_df['sentiment'].isin(['0', '1', '2'])]\n",
    "    feelings_df = feelings_df.loc[feelings_df['lang'].isin(['es', 'en'])]\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for i, row in feelings_df.iterrows():\n",
    "        sys.stdout.write(\"\\rNormalizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "        sys.stdout.flush()\n",
    "        feelings_df.at[i, 'text'] = (\n",
    "            re.sub(' +', ' ', re.sub(\"http\\S+\", \"\", re.sub('\\s+', ' ', str(feelings_df.at[i, 'text']))))\n",
    "        ).strip()\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    if balance_data == 1:\n",
    "        feelings_df.dropna()\n",
    "        feelings_df[\"Sello\"] = 0\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            #sys.stdout.write(\n",
    "             #   \"\\rCreando sellos de balanceamiento \" +\n",
    "             #   str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2))\n",
    "             #   + \"%\"\n",
    "            #)\n",
    "            #sys.stdout.flush()\n",
    "            feelings_df.at[i, 'sello'] = str(feelings_df.at[i, 'lang']) + '_' + str(feelings_df.at[i, 'sentiment'])\n",
    "        print(\"\\nBalanceando df\")\n",
    "        min_len1 = int(min(feelings_df['sello'].value_counts()))\n",
    "        df_0 = resample(feelings_df[feelings_df.sello == 'en_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_1 = resample(feelings_df[feelings_df.sello == 'en_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_2 = resample(feelings_df[feelings_df.sello == 'en_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_3 = resample(feelings_df[feelings_df.sello == 'es_0'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_4 = resample(feelings_df[feelings_df.sello == 'es_1'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        df_5 = resample(feelings_df[feelings_df.sello == 'es_2'], replace=False, n_samples=min_len1, random_state=1)\n",
    "        feelings_df = pandas.concat([df_0, df_1, df_2, df_3, df_4, df_5])\n",
    "        feelings_df = feelings_df.filter(['text', 'lang', 'sentiment'])\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if lematizacion == 1:\n",
    "        stemmer_en = SnowballStemmer('english')\n",
    "        stemmer_es = SnowballStemmer('spanish')\n",
    "        for i, row in feelings_df.iterrows():\n",
    "            #sys.stdout.write(\"\\rLematizando df \" + str(round(((i + 1) / (feelings_df.shape[0])) * 100, 2)) + \"%\")\n",
    "            #sys.stdout.flush()\n",
    "            if feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'es':\n",
    "                feelings_df.at[i, 'text'] = stemmer_es.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "            elif feelings_df.at[i, 'text'] is str and feelings_df.at[i, 'lang'] == 'en':\n",
    "                feelings_df.at[i, 'text'] = stemmer_en.stem(unidecode.unidecode(\n",
    "                    unicode(feelings_df.at[i, 'text'].lower(), \"utf-8\"))\n",
    "                )\n",
    "        feelings_df.reset_index(drop=True, inplace=True)\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Df sentiments entregado\\n\")\n",
    "    feelings_df.reset_index(drop=True, inplace=True)\n",
    "    return feelings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando df  100.0%\n",
      "Normalizando df 0.16%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipedev/Documentos/entornopruebas/lib/python3.6/site-packages/pandas/core/frame.py:6701: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 9.22%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 24.46%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 39.38%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 54.41%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 69.56%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 84.72%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando df 100.0%\n",
      "\n",
      "Balanceando df\n",
      "\n",
      "Df sentiments entregado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_feelings_df(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets en el dataframe original: 15564\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTweets en el dataframe original: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>PERIODISMO IRRESPONSABLE Van apareciendo los v...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>Ma√±ana mi√©rcoles 6 de mayo, en Autores en cuar...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15561</th>\n",
       "      <td>COVID-19 Desescalada comienzo Fase 1 el 11 de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>Coronavirus: nuevo comunicado del Comit√© de Em...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15563</th>\n",
       "      <td>che en gba no est√°n para flexibilizar nada de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang sentiment\n",
       "15559  PERIODISMO IRRESPONSABLE Van apareciendo los v...   es         2\n",
       "15560  Ma√±ana mi√©rcoles 6 de mayo, en Autores en cuar...   es         2\n",
       "15561  COVID-19 Desescalada comienzo Fase 1 el 11 de ...   es         2\n",
       "15562  Coronavirus: nuevo comunicado del Comit√© de Em...   es         2\n",
       "15563  che en gba no est√°n para flexibilizar nada de ...   es         2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir looking forward to the beats mus...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thanks to @theBeardedCripl and @hannietravels ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@SouthwestAir great cabin and flight crew this...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus and Communities: Activists step up...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@JetBlue Thanks. Still booked our trip 3/13-17...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@JetBlue gr8 #Mint crew on #flight 123 to #LAX...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@SouthwestAir Thank you for the tip!</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@southwestair SWEET!!! Glad to hear it. I'll k...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Happy #TT to my friends @AmericanAir . Hope th...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@united Thank you. Took care of everything and...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@united despite shaky connections, looks like ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@SouthwestAir Yeah, we figured it out. Thanks.</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@SouthwestAir thankyou :))‚ù§Ô∏è</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@JetBlue thanks for the response - when is the...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@AmericanAir thanks, me too</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@SouthwestAir give this guy a raise....great s...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Adapting Small Business to COVID-19 FourTen Cr...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@JetBlue thanks for the response. We are hopeful.</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@JetBlue @CinziannaP thank you! I like the qui...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@SouthwestAir CEO Gary Kelly, \"We are America'...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@JetBlue ha ha! Can I get a wake up call at bo...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@united great flight into PVD. Smallest plane ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@united thank you! Love united!! Have 4 flight...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@VirginAmerica Keep up the great work :)</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@JetBlue Really!? That's good to hear! Thanks ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@SouthwestAir happy to enter your sweepstakes ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@united we got it, thanks.</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@SouthwestAir @AmericanAir y'all are better th...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@VirginAmerica has getaway deals through May, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15534</th>\n",
       "      <td>A partir de qu√© d√≠a de la cuarentena pasa a se...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15535</th>\n",
       "      <td>Ya quiero que se termine esta cuarentena para ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15536</th>\n",
       "      <td>Como hacen para sacarse fotos lindas en cuaren...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15537</th>\n",
       "      <td>Un se√±or acaba de comprar sus tamales de yapa ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15538</th>\n",
       "      <td>A m√≠ no me importa engordar en la cuarentena ,...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15539</th>\n",
       "      <td>Ya que pase la cuarentena</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15540</th>\n",
       "      <td>Por tercera ocasi√≥n, las autoridades de Ecuado...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15541</th>\n",
       "      <td>\"Coronavirus: Librer√≠as, jugueter√≠as y otros c...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15542</th>\n",
       "      <td>Deben leerlo aunque se que algunos no lo enten...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15543</th>\n",
       "      <td>üî¥Mapa de contagios #Covid19_mx en #Cuernavacaüî¥...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15544</th>\n",
       "      <td>Termina esta cuarentena y por mi paz mental qu...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15545</th>\n",
       "      <td>Hoy precisamente que entramos en cuarentena es...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15546</th>\n",
       "      <td>¬°Es el Martes! Y a como va este 2020 es probab...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15547</th>\n",
       "      <td>Toda la cuarentena e usado short, ¬øa√∫n sabre u...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15548</th>\n",
       "      <td>Nos fumamos como 2 meses con el espa√±ol y just...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15549</th>\n",
       "      <td>Leo \"webinar\", \"webinar\"! En todo lado y juro ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15550</th>\n",
       "      <td>Cu√°ndo se termina la cuarentena, I'm harta de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>La letalidad del Covid en M√©xico al 9 de mayo:...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15552</th>\n",
       "      <td>La pandemia del coronavirus hace m√°s necesario...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15553</th>\n",
       "      <td>No se ustedes pero yo terminando la cuarentena...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15554</th>\n",
       "      <td>Le tengo m√°s miedo a los pol√≠ticos que al coro...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15555</th>\n",
       "      <td>Pa‚Äô como vamos el #coronavirus tambi√©n va a ar...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15556</th>\n",
       "      <td>HAY QUE DESTERRAR ESOS ACTOS COMO SI FUERA EL ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15557</th>\n",
       "      <td>@Claudiashein Los brotes de covid que surjan d...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15558</th>\n",
       "      <td>¬°VUELVE LA JOYA üíé ‚öΩÔ∏è! El atacante argentino Pa...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>PERIODISMO IRRESPONSABLE Van apareciendo los v...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>Ma√±ana mi√©rcoles 6 de mayo, en Autores en cuar...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15561</th>\n",
       "      <td>COVID-19 Desescalada comienzo Fase 1 el 11 de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>Coronavirus: nuevo comunicado del Comit√© de Em...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15563</th>\n",
       "      <td>che en gba no est√°n para flexibilizar nada de ...</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15564 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang sentiment\n",
       "0      @SouthwestAir looking forward to the beats mus...   en         0\n",
       "1      Thanks to @theBeardedCripl and @hannietravels ...   en         0\n",
       "2      @SouthwestAir great cabin and flight crew this...   en         0\n",
       "3      Coronavirus and Communities: Activists step up...   en         0\n",
       "4      @JetBlue Thanks. Still booked our trip 3/13-17...   en         0\n",
       "5      @JetBlue gr8 #Mint crew on #flight 123 to #LAX...   en         0\n",
       "6                   @SouthwestAir Thank you for the tip!   en         0\n",
       "7      @southwestair SWEET!!! Glad to hear it. I'll k...   en         0\n",
       "8      Happy #TT to my friends @AmericanAir . Hope th...   en         0\n",
       "9      @united Thank you. Took care of everything and...   en         0\n",
       "10     @united despite shaky connections, looks like ...   en         0\n",
       "11        @SouthwestAir Yeah, we figured it out. Thanks.   en         0\n",
       "12                          @SouthwestAir thankyou :))‚ù§Ô∏è   en         0\n",
       "13     @JetBlue thanks for the response - when is the...   en         0\n",
       "14                           @AmericanAir thanks, me too   en         0\n",
       "15     @SouthwestAir give this guy a raise....great s...   en         0\n",
       "16     Adapting Small Business to COVID-19 FourTen Cr...   en         0\n",
       "17     @JetBlue thanks for the response. We are hopeful.   en         0\n",
       "18     @JetBlue @CinziannaP thank you! I like the qui...   en         0\n",
       "19     @SouthwestAir CEO Gary Kelly, \"We are America'...   en         0\n",
       "20     @VirginAmerica @virginmedia I'm flying your #f...   en         0\n",
       "21     @JetBlue ha ha! Can I get a wake up call at bo...   en         0\n",
       "22     @united great flight into PVD. Smallest plane ...   en         0\n",
       "23     @united thank you! Love united!! Have 4 flight...   en         0\n",
       "24              @VirginAmerica Keep up the great work :)   en         0\n",
       "25     @JetBlue Really!? That's good to hear! Thanks ...   en         0\n",
       "26     @SouthwestAir happy to enter your sweepstakes ...   en         0\n",
       "27                            @united we got it, thanks.   en         0\n",
       "28     @SouthwestAir @AmericanAir y'all are better th...   en         0\n",
       "29     @VirginAmerica has getaway deals through May, ...   en         0\n",
       "...                                                  ...  ...       ...\n",
       "15534  A partir de qu√© d√≠a de la cuarentena pasa a se...   es         2\n",
       "15535  Ya quiero que se termine esta cuarentena para ...   es         2\n",
       "15536  Como hacen para sacarse fotos lindas en cuaren...   es         2\n",
       "15537  Un se√±or acaba de comprar sus tamales de yapa ...   es         2\n",
       "15538  A m√≠ no me importa engordar en la cuarentena ,...   es         2\n",
       "15539                          Ya que pase la cuarentena   es         2\n",
       "15540  Por tercera ocasi√≥n, las autoridades de Ecuado...   es         2\n",
       "15541  \"Coronavirus: Librer√≠as, jugueter√≠as y otros c...   es         2\n",
       "15542  Deben leerlo aunque se que algunos no lo enten...   es         2\n",
       "15543  üî¥Mapa de contagios #Covid19_mx en #Cuernavacaüî¥...   es         2\n",
       "15544  Termina esta cuarentena y por mi paz mental qu...   es         2\n",
       "15545  Hoy precisamente que entramos en cuarentena es...   es         2\n",
       "15546  ¬°Es el Martes! Y a como va este 2020 es probab...   es         2\n",
       "15547  Toda la cuarentena e usado short, ¬øa√∫n sabre u...   es         2\n",
       "15548  Nos fumamos como 2 meses con el espa√±ol y just...   es         2\n",
       "15549  Leo \"webinar\", \"webinar\"! En todo lado y juro ...   es         2\n",
       "15550  Cu√°ndo se termina la cuarentena, I'm harta de ...   es         2\n",
       "15551  La letalidad del Covid en M√©xico al 9 de mayo:...   es         2\n",
       "15552  La pandemia del coronavirus hace m√°s necesario...   es         2\n",
       "15553  No se ustedes pero yo terminando la cuarentena...   es         2\n",
       "15554  Le tengo m√°s miedo a los pol√≠ticos que al coro...   es         2\n",
       "15555  Pa‚Äô como vamos el #coronavirus tambi√©n va a ar...   es         2\n",
       "15556  HAY QUE DESTERRAR ESOS ACTOS COMO SI FUERA EL ...   es         2\n",
       "15557  @Claudiashein Los brotes de covid que surjan d...   es         2\n",
       "15558  ¬°VUELVE LA JOYA üíé ‚öΩÔ∏è! El atacante argentino Pa...   es         2\n",
       "15559  PERIODISMO IRRESPONSABLE Van apareciendo los v...   es         2\n",
       "15560  Ma√±ana mi√©rcoles 6 de mayo, en Autores en cuar...   es         2\n",
       "15561  COVID-19 Desescalada comienzo Fase 1 el 11 de ...   es         2\n",
       "15562  Coronavirus: nuevo comunicado del Comit√© de Em...   es         2\n",
       "15563  che en gba no est√°n para flexibilizar nada de ...   es         2\n",
       "\n",
       "[15564 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------- Lexicones\n",
    "#Lectura de SentiWordNet Obtenido de \n",
    "#https://www.nltk.org/_modules/nltk/corpus/reader/sentiwordnet.html\n",
    "\n",
    "# SentiWordNet[word] = {POS,\tID,\tPosScore,\tNegScore}\n",
    "contador = 0\n",
    "SentiWordNet = dict()\n",
    "for lines in open('Data Lexicon/SentiWordNet_3.0.0.txt'):\n",
    "    if lines.startswith('#'):\n",
    "        continue\n",
    "    line = lines.split('\\t')\n",
    "    palabra = line[4].split('#')[0]\n",
    "    if (palabra in SentiWordNet) or (palabra==''):\n",
    "        continue\n",
    "    else:\n",
    "        SentiWordNet[palabra]={'POS': line[0], 'ID': line[1], 'PosScore': line[2], 'NegScore': line[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFIN[word] = sentiment\n",
    "AFFIN = dict()\n",
    "for lines in open('Data Lexicon/AFFIN-111.txt'):\n",
    "    AFFIN[lines.split('\\t')[0]]=(lines.split('\\t')[1]).split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "def Lexicon(data):\n",
    "    addlex = list()\n",
    "    for frase in data:\n",
    "        splited = tt.tokenize(frase)\n",
    "        sum_swn_neg = 0\n",
    "        sum_swn_pos = 0\n",
    "        affin = 0\n",
    "        stnet = 0\n",
    "        word_stat = 0\n",
    "        for word in splited:\n",
    "            if word in SentiWordNet.keys():\n",
    "                sum_swn_neg += float(SentiWordNet[word]['NegScore'])\n",
    "                sum_swn_pos += float(SentiWordNet[word]['PosScore'])\n",
    "            if word in AFFIN.keys():\n",
    "                affin += float(AFFIN[word])\n",
    "            \n",
    "        addlex.append([sum_swn_neg, sum_swn_pos, affin])\n",
    "    return addlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extension(matriz, data):\n",
    "    extendLex = np.array(Lexicon(data))\n",
    "    return np.append(matriz, extendLex, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=3000, min_df=7, max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Espa√±ol.\n",
    "df_es = df[df1.lang == 'es']\n",
    "# -------- Ingles.\n",
    "df_en = df[df1.lang == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7782, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(df_en['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_en['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizacionLexicon = extension(features, df_en['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in vectorizacionLexicon: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test = train_test_split(vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelos_sentiments_supervisados(modelo_entr,vectorizacionLexicon,labels):\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    import pandas as pd\n",
    "    import pickle, os, nltk\n",
    "\n",
    "\n",
    "    modelos_en = {\n",
    "        'GBT': AdaBoostClassifier(DecisionTreeClassifier(max_depth=200), n_estimators=10),\n",
    "        'RF': RandomForestClassifier(n_estimators=100, max_depth=300),\n",
    "        'NN': MLPClassifier(hidden_layer_sizes=(50, 2), max_iter=100),\n",
    "        'DT': DecisionTreeClassifier(max_depth=16),\n",
    "        'NB': GaussianNB()\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # ---------------- Asignacion de los modelos y vectorizadores.\n",
    "    # -------- Espa√±ol.\n",
    "    #modelo_es = modelos_es[modelo_entr]\n",
    "    #vectorizer_es = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('spanish'))\n",
    "    # -------- Ingles.\n",
    "    modelo_en = modelos_en[modelo_entr]\n",
    "    #vectorizer_en = TfidfVectorizer(use_idf=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "    # ---------------- Lectura y separacion de datos.\n",
    "    #df = pd.DataFrame(get_feelings_df(df_balanceado, df_lematizado))\n",
    "    # -------- Espa√±ol.\n",
    "    #df_es = df[df.lang == 'es']\n",
    "    # -------- Ingles.\n",
    "    #df_en = df[df.lang == 'en']\n",
    "    #del df\n",
    "    print(\"Separacion de datos por idioma terminada\")\n",
    "\n",
    "    # ---------------- Separacion en data y labels de entrenamiento.\n",
    "    # -------- Espa√±ol.\n",
    "    #data_train_es, data_test_es, label_train_es, label_test_es = train_test_split(\n",
    "    #    df_es['text'], df_es['sentiment'], random_state=1\n",
    "    #)\n",
    "    #del df_es\n",
    "    # -------- Ingles.\n",
    "    data_train_en, data_test_en, label_train_en, label_test_en = train_test_split(\n",
    "        vectorizacionLexicon, labels, random_state=1\n",
    "    )\n",
    "    #print(\"Division de datos terminada\")\n",
    "    #del df_en\n",
    "\n",
    "    # ---------------- vectorizacion de los textos.\n",
    "    # -------- Espa√±ol.\n",
    "    #training_data_es = vectorizer_es.fit_transform(data_train_es)\n",
    "    #testing_data_es = vectorizer_es.transform(data_test_es)\n",
    "    #del data_train_es, data_test_es\n",
    "    #print(\"Vectorizacion en espa√±ol terminada\")\n",
    "    # -------- Ingles.\n",
    "    \n",
    "\n",
    "\n",
    "    # -------- Ingles.\n",
    "    ruta_modelo_en = './entrenamiento de modelos/modelos/' + str(type(modelo_en).__name__) + '_Sentiment_en.sav'\n",
    "    modelo_en.fit(data_train_en, label_train_en)\n",
    "    pickle.dump(modelo_en, open(ruta_modelo_en, 'wb'))\n",
    "    print(\"Modelo de \" + str(type(modelo_en).__name__) + \" en ingles guardado en \" + ruta_modelo_en)\n",
    "\n",
    "\n",
    "    # -------- Ingles.\n",
    "    predictions_en = modelo_en.predict(data_test_en)\n",
    "    print(\"Predicciones terminadas\")\n",
    "\n",
    "    # -------- Ingles.\n",
    "    print(\"\\nResultados \" + str(type(modelo_en).__name__) + \" Ingles:\\n\")\n",
    "    print(classification_report(label_test_en, predictions_en))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n",
      "Modelo de GaussianNB en ingles guardado en ./entrenamiento de modelos/modelos/GaussianNB_Sentiment_en.sav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones terminadas\n",
      "\n",
      "Resultados GaussianNB Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.70      0.61       634\n",
      "           1       0.67      0.70      0.68       675\n",
      "           2       0.61      0.41      0.49       637\n",
      "\n",
      "    accuracy                           0.60      1946\n",
      "   macro avg       0.61      0.60      0.60      1946\n",
      "weighted avg       0.61      0.60      0.60      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NB',vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de DecisionTreeClassifier en ingles guardado en ./entrenamiento de modelos/modelos/DecisionTreeClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados DecisionTreeClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       634\n",
      "           1       0.67      0.74      0.71       675\n",
      "           2       0.64      0.59      0.62       637\n",
      "\n",
      "    accuracy                           0.66      1946\n",
      "   macro avg       0.66      0.66      0.66      1946\n",
      "weighted avg       0.66      0.66      0.66      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('DT',vectorizacionLexicon,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos RF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de RandomForestClassifier en ingles guardado en ./entrenamiento de modelos/modelos/RandomForestClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados RandomForestClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74       634\n",
      "           1       0.76      0.82      0.79       675\n",
      "           2       0.71      0.71      0.71       637\n",
      "\n",
      "    accuracy                           0.75      1946\n",
      "   macro avg       0.75      0.75      0.75      1946\n",
      "weighted avg       0.75      0.75      0.75      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('RF',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos GBT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de AdaBoostClassifier en ingles guardado en ./entrenamiento de modelos/modelos/AdaBoostClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados AdaBoostClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       634\n",
      "           1       0.69      0.70      0.69       675\n",
      "           2       0.62      0.60      0.61       637\n",
      "\n",
      "    accuracy                           0.66      1946\n",
      "   macro avg       0.65      0.66      0.66      1946\n",
      "weighted avg       0.66      0.66      0.66      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('GBT',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de modelos NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Separacion de datos por idioma terminada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/felipedev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de MLPClassifier en ingles guardado en ./entrenamiento de modelos/modelos/MLPClassifier_Sentiment_en.sav\n",
      "Predicciones terminadas\n",
      "\n",
      "Resultados MLPClassifier Ingles:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       634\n",
      "           1       0.77      0.67      0.72       675\n",
      "           2       0.62      0.69      0.65       637\n",
      "\n",
      "    accuracy                           0.69      1946\n",
      "   macro avg       0.69      0.69      0.69      1946\n",
      "weighted avg       0.69      0.69      0.69      1946\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felipedev/Documentos/entornopruebas/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "entrenar_modelos_sentiments_supervisados('NN',vectorizacionLexicon,labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
