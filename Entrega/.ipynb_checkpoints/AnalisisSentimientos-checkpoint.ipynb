{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- leectura y guardado de los documentos en un data frame de pandas.\n",
    "main_dataframe = pd.DataFrame(columns=['sentiment', 'text'])\n",
    "\n",
    "dataframe_1 = pd.read_csv('./Data/Tweets0.csv')\n",
    "dataframe_2 = pd.read_csv('./Data/Tweets1.csv')\n",
    "dataframe_3 = pd.read_csv('./Data/Tweets2.csv')\n",
    "dataframe_4 = pd.read_csv('./Data/Tweets3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe_1 = dataframe_1.filter(['airline_sentiment', 'text'])\n",
    "dataframe_2 = dataframe_2.filter(['sentiment', 'text'])\n",
    "dataframe_3 = dataframe_3.filter(['sentiment', 'text'])\n",
    "dataframe_4 = dataframe_4.filter(['sentiment', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets en el dataframe original: 16140\n"
     ]
    }
   ],
   "source": [
    "dataframe_1.columns = ['sentiment', 'text']\n",
    "\n",
    "dataframe_1['sentiment'] = dataframe_1.sentiment.map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "dataframe_2['sentiment'] = dataframe_2.sentiment.map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "dataframe_3['sentiment'] = dataframe_3.sentiment.map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "dataframe_4['sentiment'] = dataframe_4.sentiment.map({'positive': 0, 'negative': 1, 'neutral': 2})\n",
    "\n",
    "main_dataframe = pd.concat([dataframe_1, dataframe_2, dataframe_3, dataframe_4])\n",
    "del dataframe_1, dataframe_2, dataframe_3, dataframe_4\n",
    "\n",
    "print(\"\\nTweets en el dataframe original: \" + str(main_dataframe.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets en el dataframe balanceado: 8010\n"
     ]
    }
   ],
   "source": [
    "# ---------------- balanceamiento del dataset.\n",
    "min_len = int(min(main_dataframe['sentiment'].value_counts()))\n",
    "df_0 = resample(main_dataframe[main_dataframe.sentiment == 0], replace=False, n_samples=min_len)\n",
    "df_1 = resample(main_dataframe[main_dataframe.sentiment == 1], replace=False, n_samples=min_len)\n",
    "df_2 = resample(main_dataframe[main_dataframe.sentiment == 2], replace=False, n_samples=min_len)\n",
    "new_main_dataframe = pd.concat([df_0, df_1, df_2])\n",
    "del main_dataframe\n",
    "\n",
    "print(\"Tweets en el dataframe balanceado: \" + str(new_main_dataframe.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- construccion de modelo de bolsa de palabras.\n",
    "count_vector = CountVectorizer()\n",
    "features = count_vector.fit_transform(new_main_dataframe['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Separacion en data y labels de entrnamiento.\n",
    "data_train, data_test, label_train, label_test = train_test_split(\n",
    "    features,\n",
    "    new_main_dataframe['sentiment']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=8,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=6, random_state=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------- entrenamiento del modelo.\n",
    "modelo = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8), n_estimators=6)\n",
    "modelo.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- implementacion del modelo.\n",
    "predictions0 = modelo.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos para la prueba: 2003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------- resultados del modelo.\n",
    "datos_totales_prueba = len(label_test)\n",
    "print(\"Numero de datos para la prueba: \" + str(datos_totales_prueba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de aciertos totales: 1259\n",
      "Precision total: 0.6285571642536195\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(label_test, predictions0)\n",
    "aciertos = sum([matrix[i][i] for i in range(matrix.shape[0])])\n",
    "precision = aciertos / datos_totales_prueba\n",
    "print(\"Numero de aciertos totales: \" + str(aciertos))\n",
    "print(\"Precision total: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos para la prueba: 2003\n",
      "Precision promedio: 0.6274006309576078\n",
      "Recall promedio: 0.6415748275219976\n",
      "\n",
      "Matriz de confusion: \n",
      "+----------+------------+------------+-----------+\n",
      "|          |   positive |   negative |   neutral |\n",
      "+==========+============+============+===========+\n",
      "| positive |        389 |         78 |       185 |\n",
      "+----------+------------+------------+-----------+\n",
      "| negative |         70 |        393 |       195 |\n",
      "+----------+------------+------------+-----------+\n",
      "| neutral  |         84 |        132 |       477 |\n",
      "+----------+------------+------------+-----------+\n",
      "\n",
      "Metricas de desempeño: \n",
      "+----------+-------------+----------+\n",
      "|          |   precision |   recall |\n",
      "+==========+=============+==========+\n",
      "| positive |    0.596626 | 0.71639  |\n",
      "+----------+-------------+----------+\n",
      "| negative |    0.597264 | 0.651741 |\n",
      "+----------+-------------+----------+\n",
      "| neutral  |    0.688312 | 0.556593 |\n",
      "+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def imprimir_metricas_matriz_confusion_multiclase(matrix, headers, total_datos):\n",
    "    print(\"Numero de datos para la prueba: \" + str(total_datos))\n",
    "    metrics = np.zeros([len(headers), 2])\n",
    "    for i in range(matrix.shape[0]):\n",
    "        precision = matrix[i][i] / sum(matrix[i])\n",
    "        recoil = matrix[i][i] / sum([matrix[y][i] for y in range(matrix.shape[0])])\n",
    "        metrics[i][0] = precision\n",
    "        metrics[i][-1] = recoil\n",
    "    precision_promedio = sum([metrics[y][0] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    recall_promedio = sum([metrics[y][-1] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    print(\"Precision promedio: \" + str(precision_promedio))\n",
    "    print(\"Recall promedio: \" + str(recall_promedio))\n",
    "    print(\"\\nMatriz de confusion: \\n\" + str(tabulate(\n",
    "        matrix,\n",
    "        headers=headers,\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\\nMetricas de desempeño: \\n\" + str(tabulate(\n",
    "        metrics,\n",
    "        headers=['precision', 'recall'],\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "imprimir_metricas_matriz_confusion_multiclase(\n",
    "    matrix,\n",
    "    ['positive', 'negative', 'neutral'],\n",
    "    len(label_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=300, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfc.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de aciertos totales: 1259\n",
      "Precision total: 0.6285571642536195\n"
     ]
    }
   ],
   "source": [
    "matrix1 = confusion_matrix(label_test, predictions)\n",
    "aciertos = sum([matrix[i][i] for i in range(matrix.shape[0])])\n",
    "precision = aciertos / datos_totales_prueba\n",
    "print(\"Numero de aciertos totales: \" + str(aciertos))\n",
    "print(\"Precision total: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de datos para la prueba: 2003\n",
      "Precision promedio: 0.7002372793392242\n",
      "Recall promedio: 0.7129572792561159\n",
      "\n",
      "Matriz de confusion: \n",
      "+----------+------------+------------+-----------+\n",
      "|          |   positive |   negative |   neutral |\n",
      "+==========+============+============+===========+\n",
      "| positive |        432 |         55 |       165 |\n",
      "+----------+------------+------------+-----------+\n",
      "| negative |         48 |        463 |       147 |\n",
      "+----------+------------+------------+-----------+\n",
      "| neutral  |         67 |        117 |       509 |\n",
      "+----------+------------+------------+-----------+\n",
      "\n",
      "Metricas de desempeño: \n",
      "+----------+-------------+----------+\n",
      "|          |   precision |   recall |\n",
      "+==========+=============+==========+\n",
      "| positive |    0.662577 | 0.789762 |\n",
      "+----------+-------------+----------+\n",
      "| negative |    0.703647 | 0.729134 |\n",
      "+----------+-------------+----------+\n",
      "| neutral  |    0.734488 | 0.619976 |\n",
      "+----------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def imprimir_metricas_matriz_confusion_multiclase(matrix, headers, total_datos):\n",
    "    print(\"Numero de datos para la prueba: \" + str(total_datos))\n",
    "    metrics = np.zeros([len(headers), 2])\n",
    "    for i in range(matrix.shape[0]):\n",
    "        precision = matrix[i][i] / sum(matrix[i])\n",
    "        recoil = matrix[i][i] / sum([matrix[y][i] for y in range(matrix.shape[0])])\n",
    "        metrics[i][0] = precision\n",
    "        metrics[i][-1] = recoil\n",
    "    precision_promedio = sum([metrics[y][0] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    recall_promedio = sum([metrics[y][-1] for y in range(metrics.shape[0])]) / metrics.shape[0]\n",
    "    print(\"Precision promedio: \" + str(precision_promedio))\n",
    "    print(\"Recall promedio: \" + str(recall_promedio))\n",
    "    print(\"\\nMatriz de confusion: \\n\" + str(tabulate(\n",
    "        matrix,\n",
    "        headers=headers,\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\\nMetricas de desempeño: \\n\" + str(tabulate(\n",
    "        metrics,\n",
    "        headers=['precision', 'recall'],\n",
    "        showindex=headers,\n",
    "        tablefmt='grid')\n",
    "    ))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "imprimir_metricas_matriz_confusion_multiclase(\n",
    "    matrix1,\n",
    "    ['positive', 'negative', 'neutral'],\n",
    "    len(label_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "AnalisisSentimientos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
